<h2 align="center">2020 - 2023 Meeting Notes: Securing Critical OSS Projects Working Group</h3>

**Meeting Times:**

Currently, every other Thursday alternating 9am and 3pm Pacific Time over [LFX Zoom](https://zoom-lfx.platform.linuxfoundation.org/meeting/95053207038?password=41a2966d-c96b-42d3-b2ee-635fc1dbaf51)

Previous Meetings: [Youtube Playlist](https://www.youtube.com/playlist?list=PLVl2hFL_zAh-cAfx6y4k-fODfbHeQzb_O)

<h2>Meeting Facilitator Rotation</h2>




* Amir Montazery, Open Source Tech Improvement Fund, Inc
* Jeff Mendoza, Kusari

<h2>Next meeting</h2>




* (June 1 Meeting) Charter - https://github.com/ossf/wg-securing-critical-projects/issues/52

<h2>Future topic ideas</h2>




1. Workgroup “workshop” - Security Scorecards Project Analysis – set for post-Caleb presentation
2. Discuss Prioritization and Categorization (dedicated time for this) – post phase I of set 
3. Flesh out scope of working group - define it and write it down 
4. Take Vote on Workgroup Charter
5. Walkthrough of Demo - Randall - Dedicated session to setting up Identifying Critical Projects process on Github (possibly 11/3) 
    * 30 minute dedicated session
6. What is the purpose of the SCP Set: who are the consumers?

The purpose of the “Set of Critical Projects'' curated by the Securing Critical Projects Working Group is to help guide the open source community in determining highly important open source projects that have been identified through research and discussion with the work group. These include but are not limited to open source projects that can benefit from additional resources, projects that are generally lacking in resources, projects that can especially benefit from third-party code reviews and audits, and projects that are widely used and adopted in modern technology and infrastructure systems. 

The intention is to help organizations, foundations, and all open source contributors looking to identify open source projects 



7. Are SCP Set and A-O 10K Projects orthogonal or should they be merged or is one a subset of the other for “purpose” / “reason”?

<h2>Key Documents</h2>




* [Process for Identifying Critical Projects](https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit) (old) 
* [Working Group Status Reports ](https://docs.google.com/document/d/1_2uRLlKDzDDespETSuyiBVv13lEsj709om56n9vjmgU)
* [Ingestion Form](https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps/prefill) - [Link](https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps/prefill)
* [First Iteration of Identifying Critical Projects]:[Link](https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit#gid=1024997528)
  
<h2>Recurring themes</h2>


* Welcome new members and quick intros 
* Project and Initiative updates
    * Criticality Score
    * Security Scorecards
    * Package Feeds
    * OSTIF Managed Audit Program
    * Identifying Critical Projects - Continue refining list of projects, discuss new additions/nominations
    * Allstar
* (Time Permitting) - Open Forum and Discussion: time for people to contribute updates, thoughts, ideas, etc.

<h2>Workgroup Working Session Tracking Table</h2>


<table>
  <tr>
   <td><strong>Meeting Date</strong>
   </td>
   <td><strong>Time Zone</strong>
   </td>
   <td><strong>Working Session Topic</strong>
   </td>
  </tr>
  <tr>
   <td>1/12/2023
   </td>
   <td>APAC
   </td>
   <td>Plan - Working Update
   </td>
  </tr>
  <tr>
   <td>1/26/2023
   </td>
   <td>EMEA
   </td>
   <td>Update Sheet - Send RFC
   </td>
  </tr>
  <tr>
   <td>2/9/2023
   </td>
   <td>APAC
   </td>
   <td>Normal Meeting
   </td>
  </tr>
  <tr>
   <td>2/23/2023
   </td>
   <td>EMEA
   </td>
   <td>Normal Meeting
   </td>
  </tr>
  <tr>
   <td>3/9/2023
   </td>
   <td>APAC
   </td>
   <td>Voting/Discussion
   </td>
  </tr>
  <tr>
   <td>3/23/2023
   </td>
   <td>EMEA
   </td>
   <td>Voting/Discussion
   </td>
  </tr>
  <tr>
   <td>4/6/2023
   </td>
   <td>APAC
   </td>
   <td>Voting/Discussion
   </td>
  </tr>
  <tr>
   <td>4/20/2023
   </td>
   <td>EMEA
   </td>
   <td>Finalize Version and Set
   </td>
  </tr>
</table>


<h3>Antitrust Policy Notice:</h3>

<p align="justify">Linux Foundation meetings involve participation by industry competitors, and it is the intention of the Linux Foundation to conduct all of its activities in accordance with applicable antitrust and competition laws. It is therefore extremely important that attendees adhere to meeting agendas, and be aware of, and not participate in, any activities that are prohibited under applicable US state, federal or foreign antitrust and competition laws. Examples of types of actions that are prohibited at Linux Foundation meetings and in connection with Linux Foundation activities are described in the Linux Foundation Antitrust Policy available at [http://www.linuxfoundation.org/antitrust-policy](http://www.linuxfoundation.org/antitrust-policy). If you have questions about these matters, please contact your company counsel, or if you are a member of the Linux Foundation, feel free to contact Andrew Updegrove of the firm of Gesmer Updegrove LLP, which provides legal counsel to the Linux Foundation.</p>

<h3>Code of Conduct</h3>

All OpenSSF activities are governed by the OpenSSF Code of Conduct, see: [Link](https://openssf.org/community/code-of-conduct/)

Please use the [2024 Meeting Notes](https://docs.google.com/document/d/1j_efLVDXGoKgfHHZbJtpBxd_Gso1ghHBdK3NfEVc15o/edit?usp=sharing)


<h2>2023-11-30</h2>


Attendees:



* Jeff Mendoza (Kusari)
* Amir Montazery (OSTIF)
* David Edelsohn (IBM)
* David C Stewart (Intel)
* Adrianne Marcum (OpenSSF)
* David Tobar (CMU Software Engineering Institute (SEI))
* Jared Miller (SAP)

Regrets:



* 

Agenda:



* New Faces
    * Dave Stewart, returning after a while. 
    * Sarah Elder, doing research around scorecard and other OpenSSF areas
    * Jared, looking through various working groups
* Project Updates:
    * Allstar: Participating in the “repository assessment” tooling alignment group
* Updates from members from other parts of OpenSSF
    * Recently OSTIF released new audit results about an eclipse foundation project Mosquitto: [https://ostif.org/mosquitto-security-audit/](https://ostif.org/mosquitto-security-audit/)
* Discussion:
    * Do we just have one list/set?
        * Currently just a list of critical projects, no list on those that are in need
    * How often is it updated?
        * About 18 months
        * We’d like to develop a more automated process to update the list
    * What do we want to do with the list/set?
        * Should we take on the role of leading the relationship between resources and getting those to critical packages?
        * Should we work towards automated creation of the set?
* Metadata brainstorming:
    * What additional metadata would be good to add to the set?
    * Solo-maintainer? yes/no
        * +1 - bar can be measured by # of maintainers (example: 0-2 maintainers)
    * Demographics of the project - geographic location of maintainer(s)? 
    * Demographics of the maintainers? New devs? Close to retirement? 
        * Absolute age
        * Relative age of participation in project
    * Track for developing contributors/maintainers? 
    * Support level: paid to maintain/develop project directly? Part of their primary vocation? Not paid or volunteer? Doing it for fun or professionally? 
        * Axis of developers working on a package solely for their own interest to developers working solely for pay.
        * 
    * Industry / domain /  where is this project used? - There are different arenas that oss is used: 
        * Currently tech-industry biased by our own areas of expertise
        * Both a  expertise problem and could be used as a metadata field
        * Industry / arena domain (finance, transportation, healthcare, military, academic, etc.)
        * Project initiated or industry initiated data?
    * How/where is the project used (non industry focused) ?
        * Database, web proxy, etc. Is this useful??
        * 
    * How can we judge size? Maintainer count? Activity? LOC? Complexity?
        * Some kind of weighted average of maintainer count and activity of those maintainers
    * Is there more available to work on, and is there enough support for contribution
    * Is it abandoned? yes/no
    * Is the project backed by a well funded company? 
        * What is the source of support
        * Foundations can be small or large as well
        * Some are obvious, many are hard to tell
    * Type of support that could be used by project:
* Types of support
    * Money: is there an entity to give it to, and would it result in progress?
    * Compute: cloud credits
    * Security auditing and reviews
    * Development support / maintainers / people power:
        * Drive-by bug reports - usually not helpful (which is why bug bounties are not always helpful - lots of bug reports that are more noise)
        * Help with bug backlogs: triage and fixing
        * Static/Dynamic analysis setup, rules. Testing suites. Things to help the project indefinitely as it grows.
        * Support that can be packaged
        * Soft support, usability, community
        * 
    * Visibility / Marketing / featured by a publication - will help attract more help

<h2>2023-11-16</h2>


Attendees:



* Jeff Mendoza (Kusari)
* Amir Montazery (ostif.org)
* Caleb Brown (Google)
* Khahil White (OpenSSF)

Regrets:



* 

Agenda:



* New Faces
    * Khahil White (OpenSSF)
        * Here to help, direct access to staff
* Timezone changes : This is now our new APAC time
* Project Updates:
    * Criticality score: getting back working after some token issues
    * Package Analysis: collecting some static analysis data for NPM
* Updates from members from other parts of OpenSSF
    * Jeff: GUAC project is moving to OpenSSF, had first community meeting as part of OpenSSF
    * Khahil: OpenSSF timeline for elected positions is being worked out
* Questionnaire update
    * 
    * Need access: [https://forms.gle/Chve4yz3f2nVxocW7](https://forms.gle/Chve4yz3f2nVxocW7) 
    * Editing done, Jeff will update form above once access figured out
    * Form access is now set so all can access it
* OpenSSF-wide MVS: 
    * WG will continue as if resource allocation and distribution is aligned with overall OpenSSF
* MVSR update
    * Working Group Draft: 
    * Jeff to work on roadmap, consensus on crawl-walk-run plan there, just need to reword.

<h2>2023-11-02</h2>


Attendees:



* Jeff Mendoza (Kusari)
* David Edelsohn (IBM)
* Adrianne Marcum (OpenSSF)
* Tobias Heldt
* Amir Montazery (ostif.org)
* Jonathan Velando

Regrets:



* 

Agenda:



* New Faces
    * Toby: Security Economics Researcher, 
    * Jonathan, [Conda-forge](https://conda-forge.org) eco system
* Timezone changes
    * Plan:  next APAC time: move 2 hours earlier, attempt to avoid conflicts with other WG meetings (this keeps the time the same for Australia east coast)
* [Evan] [Squid Caching Proxy Security Audit: 55 vulnerabilities and 35 0days](https://megamansec.github.io/Squid-Security-Audit/)
    * Seems related to the MVSR discussion.  Money quote: \
> The Squid Team has been helpful and supportive during the process of reporting these issues. However, they are effectively understaffed, and simply do not have the resources to fix the discovered issues. Hammering them with demands to fix the issues won’t get far.
* Questionnaire update?
    * 
* 
* MVSR update?
    * Working Group Draft: 
* Notes:
    * Remove friction from coordination of funders and developers
        * Upwork / Fiverr
    * 1:1 relationship
    * 1:intermediary:1 relationship
    * Many:1 relationship
    * Many:intermediary:1 relationship
    * OpenSSF hires developers directly
    * OpenSSF pools and distributes funds to freelance developers
    * OpenSSF matches funders and developers
    * OpenSSF needs to be more accessible to small developers, not complicated form and bureaucracy that only large companies or grant advisors can manage.
    * OpenSSF is source of funding or a platform for matching.
    * Potential solutions in Strategy and Prioritize in roadmap.

<h2>2023-10-19</h2>


Attendees:



* Evan Anderson (Stacklok)
* Jeff Mendoza (Kusari)
* David Edelsohn (IBM)
* Caleb Brown (Google)

<h2>Regrets:</h2>




* 

Agenda:



* New Faces
    * Evan Anderson
* Timezone changes
    * Plan: keep as is for this APAC time. Then move 2 hours earlier, attempt to avoid conflicts with other WG meetings (this keeps the time the same for Australia east coast)
* 
* Questionnaire update?
* MVSR update?
    * Potentially revisiting / changing our [charter](https://github.com/ossf/wg-securing-critical-projects/blob/main/CHARTER.md)
    * 
    *  OSSF
    * TAC: [https://github.com/ossf/tac/blob/main/technical-vision.md](https://github.com/ossf/tac/blob/main/technical-vision.md)
    * Working Group Draft: 
    * 
* [Adopting Alpha-Omega top 10k issue](https://github.com/ossf/wg-securing-critical-projects/issues/66)

<h2>2023-10-05</h2>


Attendees:



* David Edelsohn (IBM)
* Jeff Mendoza (Kusari)
* Amir Montazery (OSTIF)

<h2>Regrets:</h2>




* 

Agenda:



* New Faces
    * 
* Timezone changes
    * Plan: keep as is for next APAC time. Then move 2 hours earlier, attempt to avoid conflicts with other WG meetings (this keeps the time the same for Australia east coast)
* Questionnaire:
    * 
    * Jeff will send to Adrianne
* Ingestion form (Nominate a project you think is critical…)
    * [Ingestion Form](https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps/prefill) - Link: [https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps/prefill](https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps/prefill)
    * [https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps](https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps) 
    * Shortened Link to Form: [https://forms.gle/YspyTsuQskkUwvka8](https://forms.gle/YspyTsuQskkUwvka8)
    * 
    * Link to project page: ( ex: [https://mochajs.org/](https://mochajs.org/) )
    * Link to source repository: ( ex: [https://github.com/mochajs/mocha](https://github.com/mochajs/mocha) )
    * Link to package on package manager: ( ex: [https://www.npmjs.com/package/mocha](https://www.npmjs.com/package/mocha) ) 
    * More description on what the “project” includes / scope: ??? ( ex: “Includes main project and plugins…” )
    * 
    * Justification
    * 
    * What criteria is that based on? (Criticality Score, Quantitative Data, Qualitative Data, etc.) 
    * 
    * Freeform statistics: Package Manager Download Count (Docker, npm, gems, etc.),  [Census II](https://www.linuxfoundation.org/research/census-ii-of-free-and-open-source-software-application-libraries) z-score, [Criticality Score](https://github.com/ossf/criticality_score#public-data).
* Need to do: MSVR for working group 
    * 
    *  OSSF
    * TAC: [https://github.com/ossf/tac/blob/main/technical-vision.md](https://github.com/ossf/tac/blob/main/technical-vision.md)
    * Working Group Draft: 
* 

<h2>2023-09-21</h2>


Attendees:



* Jeff Mendoza (Kusari)
* Caleb Brown (Google)
* Victor Lu

<h2>Regrets:</h2>




* Amir Montazery (OSTIF)

Agenda:



* New Faces
    * Victor, from AI/ML security WG
* Timezone changes
    * Plan: keep as is for next APAC time. Then move 2 hours earlier, attempt to avoid conflicts with other WG meetings (this keeps the time the same for Australia east coast)
* Blog post malicious packages
    * Caleb added comment on what to say if Malicious Packages blog has not gone out first
    * If the Malicious Packages blog is already out, can mention it.
    * Jeff to email Ashwin to make sure he sees that
* Victor 
    * [https://docs.google.com/document/d/1veEdcQAqf2KhXTf7YlOW_lIif8kCDNgLGRqLqi70F44/edit#heading=h.k1viygenmlbp](https://docs.google.com/document/d/1veEdcQAqf2KhXTf7YlOW_lIif8kCDNgLGRqLqi70F44/edit#heading=h.k1viygenmlbp)
    * How can we make sure all critical projects in the AI space are included?
    * AI/ML is becoming more mainstream, which elevates related projects in criticality
    * Safety implications (as opposed to Security) can elevate AI projects in criticality as well.
    * Model security/safety is also a part (not only code / build)
    * Linked nist whitepaper (in doc) covers additional risk to AI tech compared to traditional software
    * 

<h2>2023-09-07</h2>


Attendees:



* David Edelsohn (IBM)
* Jeff Mendoza (Kusari)
* Amir Montazery (OSTIF)

<h2>Regrets:</h2>


Agenda:



* New Faces
    * No new faces
* 
    * Latest updates from us: 
    * Sent to Ashwin
*  Chat about how to proceed
    * Curation system / automated / async
    * Comment/vote system
    * Continue to treat set as good candidates for security reviews/audit
    * Need to get solid on consumers of list/set and for what
    * Our use-case gathering will result in a lot of suggestions / comments / votes
    * Ideas for handling feedback: David has asked LF staff below and they are willing to help with surveys
        * LF replied that they can help with Google Forms or SurveyMonkey polling. LF Contact: Adrianne Marcum.
    * Requirements: 
        * Automation - automatically feed into a viewable table/readout 
        * People will have feedback into 
* 
* 
    * Accompanying text for set of critical projects
    * Final Draft completed
    * Uploading to Github to accompany 2023 Set
* Need to do: MSVR for working group 
    * 
    *  OSSF
    * TAC: [https://github.com/ossf/tac/blob/main/technical-vision.md](https://github.com/ossf/tac/blob/main/technical-vision.md)
* WG needs expertise for different market segments.
    * Currently the focus is web and internet
    * Others include industrial plant, medical, power generation, automotive, transportation,  finance, military, etc.  All are critical infrastructure and all use open source software, but the critical projects do not necessarily represent those dependencies.
        * For example, is a particular Open Source astronomy package critical? For a home hobbyist use, no. For a university observatory, maybe. If used by the military, maybe it is critical.

<h2>2023-08-24</h2>


Attendees:



* David Edelsohn (IBM)
* Jeff Mendoza (Kusari)
* Amir Montazery (OSTIF)
* Caleb Brown (Google)

<h2>Regrets:</h2>


Agenda:



* New Faces
    * No new faces
* Malicious Packages update (Caleb) 
    * Checkmarx now contributing
    * GHSA malware is being imported
    * Package Analysis detections are being imported
    * Corpus is currently about 9k, with many thousands ready to be merged in PRs. (Now 15k)
* Ashwin/article content 
    * Hi Ashwin! Open Source policy work, and works with OpenSSF marketing and posts. 
    * Doing a series on various working groups
        * Highlight the work a wg has been doing
        * [https://openssf.org/blog/2023/07/27/openssf-vulnerability-disclosures-working-group-helps-guide-and-automate-handling-risk/](https://openssf.org/blog/2023/07/27/openssf-vulnerability-disclosures-working-group-helps-guide-and-automate-handling-risk/)
        * 
    * 
* Went into 
    * Almost done with draft
    * Wrap up document and get it onto the repo

<h2> 2023-08-10</h2>


Attendees:



* David Edelsohn (IBM)
* Jeff Mendoza (Kusari)
* Amir Montazery (OSTIF)

<h2>Regrets:</h2>


Agenda:



* New Faces
    * No new faces
* Critical Projects Set v1.1 tools for polling
    * LF replied that they can help with Google Forms or SurveyMonkey polling. LF Contact: Adrianne Marcum.
* Clarify “brew” in list
    * Should have a description that all “package managers” refer to the tooling: npm, pypi, brew, etc. And not the whole of all packages.
    * “Package manager infrastructure” - tooling “package manager package” and server-side software.
        * Does this include server-side infrastructure?
        * Does this include the infrastructure to build the packages that are distributed?
    * Same for alpine, doesn’t mean all software installable by alpine, but the alpine packaging and distro software.
* Next on “current game plan”
    * [Jeff to open PR] List is ready, so publish on github
        * Add new file to [https://github.com/ossf/wg-securing-critical-projects/tree/main/Initiatives/Identifying-Critical-Projects](https://github.com/ossf/wg-securing-critical-projects/tree/main/Initiatives/Identifying-Critical-Projects) directory
        * Update [https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects/Readme.md](https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects/Readme.md) to point to new list
    * Write and attach “process we used to create it” above list on github (Use DavidW’s doc: ion as starting point)
        * New Copy: 
        * Section should include “Limitations” 
            * Point out the limitations of how we came up with the set
            * Acknowledge that it is not comprehensive and can be iterated on
        * Worked on above document
    * What questions to have?
        * Open ended as above?
        * Metrics? (rate on 1-10)

<h2> 2023-07-27</h2>


Attendees:



* Caleb Brown (Google)
* David Edelsohn (IBM)
* Jeff Mendoza (Kusari)
* Josh Clements (ADI)

Regrets: David A. Wheeler - sorry, I have a conflict. However, I did some data analysis related to the list of critical projects & I share them below, hopefully you’ll find them interesting!

Agenda:



* [Caleb] [https://github.com/ossf/malicious-packages](https://github.com/ossf/malicious-packages)
    * Context: 
    * Planning on getting GHSA reports for malicious packages as well
    * Hoping to have more contribution pipelines for adding to the repo
    * Looking at OSV schema additions to add more information about the malware found
    * Working on OpenSSF blog post
    * Still a part of package-analysis, Possibly grows into its own thing, but not yet.
* David A. Wheeler: I did some quick analysis of critical projects, you might find this interesting.
    * Basically, I took the current draft list, and tried to match each project with data from OpenSSF Scorecard & OpenSSF Best Practices badge. When a match was found, I extracted a few criteria values from each.
    * I posted some draft results here: [https://drive.google.com/file/d/1cawJsldeDjK5kN_GiS0rnyO0DpJv_IpL/view?usp=drive_link](https://drive.google.com/file/d/1cawJsldeDjK5kN_GiS0rnyO0DpJv_IpL/view?usp=drive_link)
    * This was generated from:
        * Source code I wrote in Python (license (MIT OR Apache-2.0)): [https://drive.google.com/file/d/1VVBaHIIqivEwa8ncZBR4SH62gaSwAAtE/view?usp=drive_link](https://drive.google.com/file/d/1VVBaHIIqivEwa8ncZBR4SH62gaSwAAtE/view?usp=drive_link)
        * Input (CSV format): [https://drive.google.com/file/d/1KBD6Nt7RMDzGw4f0ckfFAHGgt0zQEzvE/view?usp=drive_link](https://drive.google.com/file/d/1KBD6Nt7RMDzGw4f0ckfFAHGgt0zQEzvE/view?usp=drive_link)
        * Cache (speeds execution): [https://drive.google.com/file/d/11OBJE3OGJvS93GaVfL4V90BNDwDXfwmI/view?usp=drive_link](https://drive.google.com/file/d/11OBJE3OGJvS93GaVfL4V90BNDwDXfwmI/view?usp=drive_link)
* Progress on “current game plan”
    * Looked over list in spreadsheet, looks good for publishing.
    * Todo publish on GitHub
    * Roadshow plan:
        * Here is the v1.1 Set that we created
        * This is the process we used to create it:
            * and the constraints
            * Selection
            * Discussion…
            * Common criticisms and suggestions…
    * Brainstorm: What to ask Working Groups:
        * Did you know about v1.0, were you using it?
        * Is this something you can use?
        * If so, how?
        * If not, how can it change to be useful to you in a future iteration?
        * What information would be useful?
        * What criteria would be useful for your working group - both on the input and output.
        * Ideas for collecting proposed projects to evaluate and information?
        * What thresholds for inclusion in future sets?
    * What to have for a more structured forum for async feedback, linked from github, and share a link when doing a roadshow? (ensure the questions are leading 🙂)
            * GitHub issues? - this would be public, probably not right for feedback (ok for project suggestions)
            * Google form - go with this. (standard LF polls?)
                * OKR example: [https://openssf.slack.com/archives/C019M98JSHK/p1689867523175219](https://openssf.slack.com/archives/C019M98JSHK/p1689867523175219) 
        * What questions to have?
            * Open ended as above?
            * Metrics? (rate on 1-10)
* Next on “current game plan”
    * [Jeff to open PR] List is ready, so publish on github
        * Add new file to [https://github.com/ossf/wg-securing-critical-projects/tree/main/Initiatives/Identifying-Critical-Projects](https://github.com/ossf/wg-securing-critical-projects/tree/main/Initiatives/Identifying-Critical-Projects) directory
        * Update [https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects/Readme.md](https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects/Readme.md) to point to new list
    * Write and attach “process we used to create it” above list on github (Use DavidW’s doc as starting point)
    * [David will ask LF staff for how to create] Create Google form for async feedback. SurveyMonkey?

<h2>2023-07-13</h2>


Attendees:



* Amir Montazery (OSTIF) 
* Jeff Mendoza (Kusari)
* David Edelsohn (IBM)
* Josh Clements (ADI)

 Regrets:



* 

Agenda:



* New Faces
    * Welcome Josh Clements (OSPO at ADI)
* Next Steps Moving Forward:
    * Wrap up current version - make the set the best we can with what we had
    * Then: making everything clearer - solutions. Clarify inputs, outputs esp. who they are intended to serve, criteria, process. Find a way to get more involvement & more quantitative if possible. Look at Jacques’ presentation - can we do something *without* building a fancy tool?
* X Get to finished state.
* Update: OpenSSF marketing does not plan to publicize
* More structured manner in the future
* Engage community more (road show/new version) 
* Is justification/selection needed? – the set itself says that these projects can be deemed critical, so is the justification needed
* Survey? 
    * Not just for projects, but how a set would be used. 
    * Roadshow 
        * Here’s what we did
        * Here’s how we did it
        * Can you use it as is
        * Feedback
        * Next version? 
        * What are we asking for? – 

Game plan as of 7-13-2023



1. For current set (v1.1)
    1. Clean up and put on github
    2. Go to working groups
        1. Survey
            1. Send on slack too
            2. Get feedback, engagement
2. For new version (v2.0) – incorporate all feedback from TAC/different folks
    3. Capturing metadata
        2. Process used
        3. Limitations
        4. Intended Use
        5. Definitions and Units
            3. Foundations - large collections of projects 
            4. Projects - can be a collection of packages, but a project
            5. Packages

<h2>2023-06-29</h2>


Attendees:



* Seth Larson (PSF)
* David Edelsohn (IBM) - works on Core Toolchain Initiative CTI
* Caleb Brown (Google)
* Amir Montazery (OSTIF) 
* David A. Wheeler (LF)

 Regrets:



* Jeff Mendoza (Kusari) - Caught a cold

Agenda:



* New Faces
    * Welcome Seth!
    * Gave Seth wg quick intros for context
* [Caleb] [https://github.com/ossf/malicious-packages](https://github.com/ossf/malicious-packages)
    * Context: 
    * [Caleb] Ready to start accepting OSV
    * PSF is working on an ingest to be able to receive this data. See: [https://discuss.python.org/t/pypi-malware-detection-project/28222](https://discuss.python.org/t/pypi-malware-detection-project/28222)
    * [DavidAW] Would be great for this to be consumed by consumers of OSV.
        * [Caleb] Yes, this is the next stage of the plan
        * [DavidAW] Have we talked to the "repos"? (e.g. npm, pypi, rubygems)
            * Briefly with Dustin, but not yet.
            * Very keen for this to be consumed by repos, but we will probably need to prove the quality of the data
    * [Seth] Reducing mean time to resolution is important.
    * [Seth] PyPI work in this area: [https://discuss.python.org/t/pypi-malware-detection-project/28222](https://discuss.python.org/t/pypi-malware-detection-project/28222)
        * API for researchers to submit reports programmatically
        * [Caleb] Will take a look and contribute
    * [Seth] Enough sources and enough data of quality, can start to automate corroborating high-quality reports from trusted sources.
    * [Caleb] Dependency confusion could be interesting
        * [DavidAW] PURLs can handle this (in theory). In practice...
* Feedback on 
    * Jeff’s notes
        * I believe only the first tab of the spreadsheet was copied, which was just the previous list. All the “Yes” projects from the second tab need to be added
        * The set is not the “Top” or “Most Critical” projects. It is simply a set of projects that have been deemed critical by the WG. It is not exhaustive. This should be communicated in the intro.
        * Due to the above, any criticism of omissions are invalid. We’ll need to have a clear suggestion/submission process outlined.
    * Seth Larson’s notes
        * Language ecosystem packaging tooling, should these be a separate category/under “Supply Chain Tools”? Tools seem disadvantaged in many of the selection criteria.
        * Where do transitive dependencies fit in this?
    * David A. Wheeler:
        * Thanks so much for the work done! We expect Alpha-Omega to use this once ready & probably others will also.
        * Don’t want to post on a blog or market with current state (yet). Need to find a way to get bigger buy-in support.
        * Propose:
            * Let’s fix what we have - Alpha-Omega, etc., will want to use it!
            * Let’s talk about what can we do next to get broader buy-in & more quantitative
        * David E: It has to be somebody
            * Can we figure out a data-driven process?
            * Need to be clearer about the input & output of this.
            * Who’s the consumer? What do they want? CISA, Alpha-Omega, etc?
            * David W: Also be clearer about the criteria & the process
            * Maybe we need different facets and ecosystems, e.g.:
                * Toolchain
                * Package managers/repos/ registries
                * Embedded
                * Linux
                * *BSD
                * (Different ecosystems, e.g. JavaScript, Python)
                * Low-level (“not name brand”) dependencies vs tent-pole, brand name, “consumer”, famous projects
            * Motivation for participation: money, power, influence.
            * Who are the consumers?
            * What is the benefit of participation and benefit of appearing on the list to encourage participation in the decision making process.
            * Rewards such as more GSoC slots, Alpha-Omega funding, OpenSSF/LF summit invites.
            * PSF can utilize resources, but so can “libnebraska”.
        * Note: People *are* using the critical projects set. The Scorecard group runs a weekly scan of 1.2M projects, & just verified that every project in the latest set hosted on GitHub is included in their weekly scans.
        * **Action items for group to consider**:
            * Wrap up current version - make the set the best we can with what we had
            * Then: making everything clearer - solutions. Clarify inputs, outputs esp. who they are intended to serve, criteria, process. Find a way to get more involvement & more quantitative if possible. Look at Jacques’ presentation - can we do something *without* building a fancy tool?

<h2>2023-06-15</h2>


Attendees:



* Jeff Mendoza (Kusari)
* Amir Montazery (OSTIF)
* David A. Wheeler (LF)
* 

Regrets:



* David Edelsohn

Agenda:



* New Faces
    * 
* Discussion on describing what the list is for, who are the users?
* Firstly: What is the list?

The purpose of the “Set of Critical Projects'' is to help guide the open source community in determining highly important open source projects that have been identified through research and discussion with the Securing Critical Projects Working work group. These include but are not limited to open source projects that can benefit from additional resources, projects that are generally lacking in resources, projects that can especially benefit from third-party code reviews and audits, and projects that are widely used and adopted in modern technology and infrastructure systems. 

* Secondly: Specific users of this list:
    * Alpha-Omega
        * Uses list to identify projects as candidates for Alpha and Omega
    * Other organizations that fund OSS improvements but would like to identify the most critical OSS, e.g., Sovereign Tech Fund (German Government), US Government (such as CISA), Open Tech Fund, and many others.
    * Sos.dev: Provides small funds for OSS projects, but would like to prioritize the more critical ones.
    * Future MFA distributions
            * Uses list to identify projects to reach out to distribute MFA devices.
    * Organizations that evaluate OSS, e.g., OSTIF 
    * Others that we don’t know about yet
* Preparing Set for publishing
    * Need a short document explaining the process that was used for selection, one they can point to. Need to have a ready answer for “How did you make this list?” & acknowledge weaknesses in the current set (varying size, some you can argue either way).
        * Ex: [https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects/Process-Document.md](https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects/Process-Document.md) Is this up to date? - no, it needs work
        * We need a different document, can pull from that. It could be just a paragraph or two.
    * Content:  
    * Do we need something nice-looking?
        * Best guide: example: [https://best.openssf.org/Concise-Guide-for-Developing-More-Secure-Software](https://best.openssf.org/Concise-Guide-for-Developing-More-Secure-Software)
        * Publish a simple bullet list? 
    * Process options:
        * Link to the existing Google docs -  
        * Generate PDF generate straight from Google docs (doesn’t have intro)
        * Create a nicer doc (Intro + list)
            * PDF
            * HTML
                * There are many ways to do this, e.g., “Simplest Possible Process” [https://github.com/ossf/tac/issues/176](https://github.com/ossf/tac/issues/176)
            * If we create markdown first, there are many ways to generate the markdown
* Blog / Announcement Plan
    * Blog Post: 
    * Intro to list description
    * Blog points:
        * Describe above: What is it? How was it made? Who is using it?
            * Content: 
        * How might you use it?
            * Are you looking to engage Open Source to improve security?
            * Are you looking to fund Open Source projects?
            * Are you looking to do security research?
            * …
            * Content: 
        * How can you get involved
            * Join the WG
            * Suggest projects
            * Content: 
    * Announcement Plan
        * OpenSSF Blog Post
* Feedback/Suggestion ideas:
    * Google forum
    * GitHub issues
    * World Writable Google Sheet (add a line)

<h2>2023-06-01</h2>


Attendees:



* Jeff Mendoza (Kusari)
* Caleb Brown (Google)
* David Edelsohn (IBM)

Regrets:



* 

Agenda:



* New Faces
    * 
* Voting/Discussion
    * 
    * Finished consensus on all but last two (which are waiting on further info)
    * Discussed removing some from current list, not sure if we can do that.
* [https://github.com/ossf/wg-securing-critical-projects/issues/66](https://github.com/ossf/wg-securing-critical-projects/issues/66)
    * Will ask Jonathan to do a presentation on what it takes to generate and/or maintain the list (What are we signing up for!?)
* Proposal from Jeff: Use new GitHub teams ([https://github.com/orgs/ossf/teams/wg-securing-critical-projects](https://github.com/orgs/ossf/teams/wg-securing-critical-projects)) to represent everyone that is a member of the WG. Will be anyone who participates in meetings.
    * Right now I can’t add people to the team that are not already in the OpenSSF org, nor can I add them to the org. People can’t add themselves, and need to be invited?
    * Jeff will follow up with PM team to figure out how to accomplish this and update the team.
* 

<h2>2023-05-18</h2>


Attendees:



* Jeff Mendoza (Kusari)

Regrets:



* Amir - joining late
* 

Agenda:



* New Faces
    * 
* Voting/Discussion
    * 

<h2>2023-05-04</h2>


Attendees:



* Jeff Mendoza (Kusari)
* Amir Montazery (OSTIF)
* David Edelsohn (IBM)
* Caleb Brown (Google)

Regrets:



* 
* 

Agenda:



* New Faces
    * 
* Having a clear consumer for our list(s) would help focus our efforts:
    * Alpha
    * Audit program
    * Great MFA project 
    * Ideas?
        * Roadshow: present to each WG about the lists, ask if they are already using it, or how they might use it?
        * Blog post about our lists
* **PURPOSE OF SET OF CRITICAL PROJECTS**:
    * The purpose of the “Set of Critical Projects'' curated by the Securing Critical Projects Working Group is to help guide the open source community in determining highly important open source projects that have been identified through research and discussion with the work group. These include but are not limited to open source projects that can benefit from additional resources, projects that are generally lacking in resources, projects that can especially benefit from third-party code reviews and audits, and projects that are widely used and adopted in modern technology and infrastructure systems. 
* Voting/Discussion
    * 

<h2>2023-04-20</h2>


Attendees:



* David Edelsohn (IBM)
* Jeff Mendoza (Kusari)
* David A. Wheeler (LF)
* Dave Stewart (Intel - Security & Privacy Lead for our SW Products org)
* Jonathan Leitschuh (Alpha Omega)
* Marco Benatto (Red Hat)
* Michael Scovetta (Microsoft)
* 

Regrets:



* Amir Montazery (OSTIF) - Kubecon EU
* 

Agenda:



* New Faces
    * 
* PTAL - to consider soon
    * Adopt the Alpha-Omega 10k critical OSS Projects list
    * [https://github.com/ossf/wg-securing-critical-projects/issues/66](https://github.com/ossf/wg-securing-critical-projects/issues/66)
    * [https://lists.openssf.org/g/openssf-wg-securing-crit-prjs/topic/adopt_the_alpha_omega_10k/97530102](https://lists.openssf.org/g/openssf-wg-securing-crit-prjs/topic/adopt_the_alpha_omega_10k/97530102)
    * **Please take a look and leave comments/questions in the issue or email thread.**
    * If accepted it’ll be a SIG (mostly NOT code), but not one that meets separately
* Voting/Discussion
    * 

<h2>2023-04-06</h2>


Attendees:



* Jeff Mendoza (Kusari)
* Amir Montazery (OSTIF)
* David A. Wheeler (LF)

Regrets:



* 

Agenda:



* New Faces
* 
* Voting/Discussion
    * 

8

<h2></h2>


Attendees:



* Jeff Mendoza
* David Edelsohn
* Amir Montazery
* Benjamin Schmidt
* Georg Kunz (Ericsson)
* Dave Stewart (Intel - Security & Privacy Lead for our SW Products org)
* Randall T. Vasquez
* Jack K
* Jeffrey Borek
* Mattia Rizzolo (Reproducible Builds/Debian)
* Josh Buker
* Shy Observer

Regrets:



* 

Agenda:



* New Faces
    * Dave Stewart, haven’t been in a while. Welcome Back!
    * Benjamin Schmidt, looking to join and contribute. Welcome!
    * Mattia Rizzolo, looking to collaborate with some OpenSSF WG. Welcome!
    * Georg Kunz, following various OpenSSF areas, looking to get an overview of our WG. Welcome!
* Meeting time update: in 2 weeks (4/6 and later) our APAC-friendly time will change to the previous DST time.
* Voting/Discussion
    * 

<h2></h2>


Attendees:



* Jeff Mendoza
* Caleb Brown
* Annapurna Veeramachaneni
* Amir Montazery
* Noah
* Michael Scovetta
* David Edelsohn
* David Wheeler

Regrets:



* 

Agenda:



* New Faces
    * Annapurna Veeramachaneni from AO
    * Michael Scovetta from A-O, welcome back
    * Noah Spahn
* Meeting time update: in 4 weeks (4/6 and later) our APAC-friendly time will change to the previous DST time.
* Voting/Discussion
    * 

<h2></h2>


Attendees:



* Jonathan Leitschuh (Alpha Omega)
* Jeff Mendoza
* Amir Montazery (ostif.org)
* David Edelsohn (IBM)bryancounselinginc

Regrets:



* 

Agenda:



* New Faces
    * none
* Schedule Update
    * Starting voting/discussion at next meeting: 3/9
    * Sent out RFC, should we send out a notice about the voting happening?
        * Which WG:
            * From last meeting:
            * Vulnerability Disclosures
            * Securing Software Repositories
            * End Users
        * Also on #general in Slack
    * Go through GitHub issues and close all that were added to v1.1 proposals list.
        * 23 -> 18 in this meeting
    * 
* How should we handle suggestions once we release v1.1?
    * GitHub issues:
        * We would need to manually take suggestions and add to v1.2 candidates sheet
    * New Sheet/Tab
        * Would need to give edit access to anyone asking to make a suggestion
        * Columns would help show what we want with a suggestion (url, license, download counts, etc.)
* Project Updates
    * Allstar
        * Operations funding secured
        * Working on a memory leak
        * Will merge contributor ladder PR. [https://github.com/ossf/allstar/pull/339](https://github.com/ossf/allstar/pull/339)  
    * OSTIF
        * Released impact report - OpenSSF blog
        * [https://openssf.org/blog/2023/02/01/independent-security-audit-impact-report/](https://openssf.org/blog/2023/02/01/independent-security-audit-impact-report/)	
* Town Hall?
    * No submission yet
    * Looks like date is before 2nd voting meeting, would be good to have a slot to ask for participation, Jeff will look into getting on the agenda if still possible.
* WG blog
    * Plan on doing one after list is released

<h2></h2>


Attendees:



* David A. Wheeler (Linux Foundation)
* Jeff Mendoza
* Caleb Brown (Google)

Regrets:



* Amir
* David Edelsohn

Agenda:



* New Faces
    * 🙁
* Schedule updated for working sessions (above table)
    * We had 2 working sessions, have polished list with updates.
    * We couldn’t send out RFC last meeting, still needs some work. Was sent out this week.
    * We’re delaying things one meeting
* RFC sent out. Ideas for further reach?
    * [https://lists.openssf.org/g/openssf-wg-securing-crit-prjs/topic/set_of_identified_critical/96796664](https://lists.openssf.org/g/openssf-wg-securing-crit-prjs/topic/set_of_identified_critical/96796664)
    * [https://openssf.slack.com/archives/C019M98JSHK/p1675727340203179](https://openssf.slack.com/archives/C019M98JSHK/p1675727340203179) 
    * OpenSSF Announce??
    * Which OpenSSF WGs are related & should be emailed (besides our own)? Best to send out simultaneously so that duplicates might be removed:
        * Vulnerability Disclosures
        * Securing Software Repositories
        * End Users
* Project updates
    * Current status of updating the critical projects list
    * Allstar: Request for infrastructure, and/or petition TAC for funding ([https://github.com/ossf/tac/blob/main/working-group-abilities.md#technical-infrastructure](https://github.com/ossf/tac/blob/main/working-group-abilities.md#technical-infrastructure))
        * Example request: [https://github.com/ossf/tac/issues/132](https://github.com/ossf/tac/issues/132) 
        * Proposed request text:
            * [Funding Request] - Allstar community instance infrastructure costs
            * Pursuant to the guidance here ([https://github.com/ossf/tac/blob/main/working-group-abilities.md#technical-infrastructure](https://github.com/ossf/tac/blob/main/working-group-abilities.md#technical-infrastructure)) The SCP WG is requesting funding for technical infrastructure for the Allstar project.
            * Allstar (https://github.com/ossf/allstar) is an app for securing GitHub repositories. The project consists of a codebase for self-hosting, and a public app instance that anyone may use (https://github.com/apps/allstar-app).
            * The project has been active for nearly 2 years, and the public app instance has 349 installs on 281 repositories, including SigStore for example.
            * While we proudly proclaim "Instance of Allstar run by OpenSSF", it has come to my attention that the current infrastructure is privately funded with stipulations to retain access to the infrastructure, - that is, they must have administrative access to the instance, with the ability to read and use secrets maintained by Allstar. However, Allstar is granted elevated permissions and data access by the users that install it on their GitHub Organizations. We’d prefer to run in a situation where the provider promises to not access this sensitive data. We request funding for neutral infrastructure to provide a truly community-run app instance. The current bill on Google Cloud Platform is around $300/month, and we don't see this increasing significantly.
    * Contributor ladder PR: [https://github.com/ossf/allstar/pull/339](https://github.com/ossf/allstar/pull/339) 
        * Adopt for Criticality Score after finalized
        * After both projects are using it then inform the TAC.
    * Package Analysis Update (Caleb)
        * Google has been talking with Checkmarx, e.g., unwrapping. Also collaborating on how to share detections.
        * We’d like to consume public detections of problems, & want to encourage other organizations who are doing this work to publish their work, as that helps everyone.
        * We’re big on sharing the data publicly.
        * We’re thinking about how to use OSV to do that.

<h2></h2>


Attendees:



* Jeff Mendoza
* David Edelsohn
* Matt Rutkowski (IBM)
* 

Agenda:



* New Faces
    * Namita
    * Rusydy
    * Rahul Gupta (Microsoft)
* Primary Goal for Meetings until 3/23/2023 - V1.1 of Critical Projects Set
    * See tracking table above. 
* Spreadsheet candidates updated
    * Jeff will fix formatting and work with Amir to send out RFC
    * 

<h2></h2>


Attendees:



* Randall T. Vásquez (Gentoo/Homebrew/SKF)
* Jeff Mendoza (Google)
* Amir Montazery (ostif.org)
* Nathan Naveen
* David Edelsohn (IBM)

Regrets:



* Antony Faris (SKF)

Agenda:



* New Faces
    * 
* Primary Goal for Meetings until 3/23/2023 - V1.1 of Critical Projects Set
    * See tracking table above. 
* V1.1 is going to be a Google Sheet - revised from v1.0 
    * V2.0 will have the github capabilities
    * V1.1 Sheet was created  - link: 
    * Have a section for projects like “Docker” that are indeed critical but maybe not necessarily open source

<h2></h2>


Attendees:



* David A. Wheeler (Linux Foundation)
* Jeff Mendoza
* Nathan Naveen
* Amir

Regrets:



* Caleb - Australia time
* Amir will be late

Agenda:



* New Faces
    * Nathan Naveen, just started contributing to criticality score, Welcome!!
* Meeting time reminder
    * Dec 29th: canceled
    * Jan 12th: APAC-friendly time
    * Jan 26th: EMEA-friendly time
* I am Nathan, a new contributor to Criticality Score.
    * I have been creating PRs and would appreciate reviews, [https://github.com/ossf/criticality_score/pulls/nathannaveen](https://github.com/ossf/criticality_score/pulls/nathannaveen).
        * David: Nag Caleb?
        * Jeff: Often helpful to get more people to commit to doing reviews. Could do a “call for help” to other OpenSSF WGs/projects to see if we can find some.
        * Basically, we need more reviewers.
    * I'm also blocked because of the issue: [https://github.com/ossf/criticality_score/issues/281](https://github.com/ossf/criticality_score/issues/281).
        * Need to get existing developer - probably Caleb
* David: Updated list of critical projects - timeline?
    * Let’s call the current list “V1”, the new system “v2” (google form), with data from various sources. v2 is taking longer than originally expected.
    * We could just do an update of V1 - same process, update list. E.g., v1.1 or date release. It won’t be done this year.
    * David: I think we need to set a date, and work backwards to determine what needs doing when. It’d be much easier to start with v1, and then have people propose deltas.
* New Version - Version Control - V.1.1 / <date>
    * Go through set
        * What should stay
        * What should go
        * What to add
    * Collaborate on a set of proposed deltas - have group precreate Google sheet
        * Discuss in the workgroup meeting
        * For each, have a column of a justification for its addition/removal (or capturing all sides)
        * Simple solution: Just use the existing sheet with a new tab:
        * Proposed: End up with a new tab of V1.1
    * That won’t eliminate v2, but we need something updated in the shorter timeframe
    * Think about timeline:
        * Work on existing suggestions, put in new sheet, name & url, optional: crit score, license, language, download count
            * 2-3 meetings
            * Objective - A cleaned up new set to put out
            * Requirements on what can be nominated
                * If its an amalgamation of projects
                * If it’s a distro
                * Single repo? 
        * Send out comms that suggestions are listed and will be considered in the future
            * People add comments/thoughts ahead of time
            * 4 weeks (skip one meeting)
        * Have actual discussion meetings in or out
            * 2 meetings
* Possible changes for v1:
    * For v1, should we have 2 URLs (home page & repo page), since they aren’t always the same?
    * Should we add a column for criticality score (commit popularity)?
* Need to update our GitHub directories.
* Action Item: Clean up Candidate Projects

<h2></h2>


Attendees:



* Jeff Mendoza (Google)
* Caleb Brown (Google)
* David Edelsohn (IBM)
* Amir Montazery (ostif.org)
* Jacques Chester (Shopify)
* Randall T. Vasquez (Gentoo)
* 

Agenda:



* New Faces
    * Thank you for all the contributions Jacques! 
* Meeting time reminder
    * Dec 15th: EMEA
    * Dec 29th: canceled
    * Jan 12th: APAC
* Criticality Score is running in Prod
    * Very exciting! Criticality score can now run “at scale” and analyze more projects to be able to analyze and determine the critical ones at a larger scale
    * Will people gamify the score and try to create a competition?
    * External publicity (Hacker News, Phoronix, The Register) should not affect the score, but to be verified.
    * Regarding publicity: 
        * OSTIF will incorporate criticality score of critical projects they worked on in 2022 as part of their reporting
        * Google planning to present score in .dev
    * Capturing and tracking changes to the definition of CS over time - i.e how criticality scores are presented in isolation from the CS code
    * Output: [https://commondatastorage.googleapis.com/ossf-criticality-score/index.html](https://commondatastorage.googleapis.com/ossf-criticality-score/index.html)
* Updates from Randall: going to send a link for the current working demo
    * One of the next meetings: Do a walkthrough of a the working demo and discuss
* Call for comments and feedback on charter ? [https://github.com/ossf/wg-securing-critical-projects/pull/58](https://github.com/ossf/wg-securing-critical-projects/pull/58)
    * Should make sure to identify what our WG does
    * What is the intent?
    * Intent of the output?
        * Clarify that we’re not “ranking” 
    * Differentiation between goals of our working group and goals of SIGs and other working groups
    * Defining what a critical project is
        * We don’t exactly define it 
        * Here are various factors one may consider to influence how to calculate criticality
        * In context of the investment: log4shell cost billions in lost productivity -> these projects have a criticality level similar to that in terms of {how criticality was calculated}
    * What does catastrophe look like? - Jacques example
        * If the water supply to a major city shuts down? Significant disruption. Potential “post 9-11” type legislation that might make things worse (shoes off before a flight example) 
    * Randall: should we be more specific? - people aren’t very responsive to a “set” 
        * Goal - 1st get a set of projects - “100 greatest hits” 
            * Then prioritize/analyze - “100 greatest rock hits” and “100 greatest pop hits”
    * Formally define how we derived the set, what was considered, make definitions 
    * Start with a high-level document 
        * What the 1st deliverable of that output would look like that
    * FOR NEXT MEETING: Dedicated time to work on high level doc aka Process for Identifying Critical Projects
        * Idea for dated release: high level summary doc for “v1” 
            * V1 is 2021/2022 effort link: 
    * Should we version our lists? - versions are basically the dates/year
        * V1 directory
        * V2 directory 
        * Current doc in gh markdown 
        * Output: json and yaml 
            * Generate gh releases in markdown 

<h2></h2>


Attendees:



* Jeff Mendoza (Google)
* Munawar Hafiz (OpenRefactory)
* Amir Montazery (ostif.org)
* Aaron Wislang (Microsoft)
* Randall T. Vasquez (Gentoo)
* 

Agenda:



* New Faces
    * 
* Updates on ingestion engine
    * Caleb got criticality score running in docker container, unblocked Randall
    * Randall will focus next week
* Vuln-disclosure WG: met with Arun from Intel. He is also interested in a criticality list with multiple entities creating lists, and counting how many times projects show up on lists. Suggested he join our WG effort.
    * We have seen that orgs are not keen on sharing a list of what they use, and find critical
* Aaron can help connect with folks from npm side/github 
* Does GitHub capture/produce data that conveys criticality?
    * Criticality score takes into account stars/activity.
    * We have been relying on things like package managers that have public download counts.
    * We’d love to consume any new data available
* Should we run through our existing list again and re-”certify” it, add/remove some amount of projects?
    * Good for feedback
    * Would it be helpful to know the intended use of our list?
        * Ex: cert group wants to help projects that are under-resourced
        * “Criticality” is somewhat independent of the usecase
        * 
* Restore spreadsheet version?
    * [https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM](https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM)
* Randall will turn on markdown linter on our WG repo

<h2></h2>


Attendees:



* Jeff Mendoza (Google)
* Caleb Brown (Google)
* Amir Montazery (ostif.org)
* Henri Yandell (AWS)
* Munawar Hafiz (OpenRefactory)
* Rob Underwood

Regrets:



* Jeff - need to leave halfway through

Agenda:



* New Faces
    * No new faces. 
* [Jeff] Meeting time reminder
    * Nov 17th EMEA
    * Dec 1st: new APAC time (after time change) - not on cal yet
        * Post DST: 7am Sydney, 12pm SFO, 3pm NYC, 8pm GMT, 8pm London
    * Dec 15th: EMEA
    * Dec 29th: canceled
    * Jan 12th: APAC
* Housekeeping: keep meeting times going into next year. 
    * Caleb happy to have APAC time for APAC folks to participate once a month at least. 
    * Lower attendance at APAC time - possibly due to confusion and getting used to new times. We’ll keep track going into 2023
    * Good idea: reminder on slack on APAC days. : Jeff will remind on 30th.
    * Overall we’re happy with time frames and will continue to meet on set times. 
    * Action Item: Confirm with EMEA group about meeting times, gather feedback, etc. 
* Jeff and Amir agree on WG Charter. Giving this meeting and 11/7 meeting opportunity for feedback and objections. 
    * Workgroup has until end of meeting 11/7 for feedback and objections. 
    * [https://github.com/ossf/wg-securing-critical-projects/pull/58](https://github.com/ossf/wg-securing-critical-projects/pull/58)
* Project Update: 
    * Criticality Score - talking about how to use criticality score for github action. 
        * Github action
            * Commit waiting
        * Deprecate python once and for all
            * Commit waiting
        * Productionizing the criticality score collection.
            * Collect data frequently 
        * Question about criticality score: How representative is the data for CS to the whole ecosystem? 
            * A: representative of oss projects on github. Oriented towards github at the moment. Covers a bulk of the ecosystem at the moment. Scores are biased to popularity and activity. Not always the most important. 
            * Looking at extracting gitlab repositories from deps.dev data
        * Question: Public Digestion of Criticality Score Data - API? Web page to view top 100? 
            * A: Yes. Idea is links will be in gh repo to public data.
            * Have in bigquery and via csv in google storage 
            * Long term wish list - a website. 
            * [https://commondatastorage.googleapis.com/ossf-criticality-score/index.html](https://commondatastorage.googleapis.com/ossf-criticality-score/index.html)
                * Folder has an updated run from June 2022
        * Question: Ability to filter projects by industry? i.e financial services? Healthcare, etc
            * A: more of a long term idea. Very dependent on companies sharing their dependencies and providing insight into what they’re using. 
                * SBOM sharing is an idea
                * Has to be done in a way that preserves privacy. 
                * Having the data would be super valuable in providing an accurate criticality score and industry usage.
        * Related - [https://security.googleblog.com/2022/10/announcing-guac-great-pairing-with-slsa.html](https://security.googleblog.com/2022/10/announcing-guac-great-pairing-with-slsa.html)
    * Package analysis
        * [https://github.com/ossf/package-analysis](https://github.com/ossf/package-analysis)
        * Currently being worked on. Google has a new person working on it. 
        * Improving and revamping data structure.
        * Introducing static analysis.
    * New release: Security Audit of Python-TUF link: [https://ostif.org/our-audit-of-python-tuf-is-complete-multiple-issues-found-and-fixed/](https://ostif.org/our-audit-of-python-tuf-is-complete-multiple-issues-found-and-fixed/)

    [https://ostif.org/our-audits-of-jackson-core-and-jackson-databind-are-complete/](https://ostif.org/our-audits-of-jackson-core-and-jackson-databind-are-complete/)

* Hen: Awesome - loving the Jackson one :)

<h2></h2>


Attendees:



* Randall T. Vasquez (Gentoo)
* Marco Benatto (Red Hat)
* Jeff Mendoza (Google)
* Amir Montazery (ostif.org)
* Antony Faris (MTHRSHP)
* Peter Singh (Astro)
* Jay White (Microsoft)
* Aaron Wislang (Microsoft)

Regrets:



* Jacques Chester (Shopify)

Agenda:



* New Faces
    * Aaron Wislang from Microsoft, works on Linux, open source, and Go. Welcome!
    * 
* [Jeff] Meeting time reminder
    * Nov 3rd: APAC
    * Nov 17th EMEA
    * Dec 1st: new APAC time (after time change) - not on cal yet
        * Post DST: 7am Sydney, 12pm SFO, 3pm NYC, 8pm GMT, 8pm London
* WG group/list reminder:
    * [https://groups.google.com/g/wg-securing-critical-projects](https://groups.google.com/g/wg-securing-critical-projects) is deprecated
        * Still used for automatic access to this doc, how do other groups handle this?
    * [https://lists.openssf.org/g/openssf-wg-securing-crit-prjs](https://lists.openssf.org/g/openssf-wg-securing-crit-prjs) is the official mailing list
* Charter vote?
    * No changes in the last few meetings. 
    * We’re okay with it overall….
    * The charter is what the norms are for the working group. 
    * Jeff and Amir - with workgroup consensus - can “ratify” the charter 
    * AI: Jeff and Amir to agree on charter - 
        * Workgroup can provide feedback, raise concerns
* Should we move “[Process for Identifying Critical Projects](https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit)” to markdown in our [WG repo](https://github.com/ossf/wg-securing-critical-projects/tree/main/Initiatives) ?
    * Also needs to be updated to describe git repo based ingestion (instead of Google form)
    * YES - move documentation to github for repo 
    * AI: Amir to update gh repo with process
        * Updated. See link: https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects/Process-Document.md
* Update From Randall - Criticality score
    * Randall and Caleb met a few times to do iterations and updates
    * Describe how the github repo would work 
        * Caleb and Randall to add to the repo 
        * Quantitative data - tool to pull the data from criticality score - use that 
        * Pull information - people run PR for a project - we run that - get the info - add that to the PR as a comment. 
* Using github for ingestion engine and to keep track of identified projects. 
    * Maintain the set using github 
* Ideas: 
    * Tagging issues - so tag an issue that’s a reco 
    * Template - use a template to keep it consistent 
    * Github action - automate 
    * Read-only: static site somewhere else just a read only of the output 
    * Not accepting issues - just accepting PRs - leave PRs open and if they get approvals then it is merged. 
        * +1 maybe not accepting issues 
    * Make it a separate repo since it’s a dedicated function/process
* Dedicated session to build POC (at an APAC friendly meeting time so Caleb can join) 
    * Aiming for 11/3/2022

<h2></h2>


Attendees:



* Jeff Mendoza (Google)
* Caleb Brown (Google)
* Jay White (Microsoft)
* David A. Wheeler (Linux Foundation)
* Randall T. Vasquez (Gentoo)
* David Edelsohn (IBM)
* 

Regrets:



* Jacques Chester (Shopify)
* Amir: EMEA timezone currently

Agenda:



* New Faces:
    * 
* Charter:
    * Not enough for quorum??
        * David: There’s no specific quorum # for a WG as far as I recall, but we do want to see multiple different organizations.
* How to incorporate criticality score tool into our process
    * Determine if tool goals are diverging from group goals?
    * It’s unlikely that the tool will produce data that exactly meets what people expect; we will need manual review. That said, we do want the tool to provide some data
    * Source Repo: [https://github.com/ossf/criticality_score](https://github.com/ossf/criticality_score)
    * Strong signal of contributor count [David A. Wheeler]
        * Not having participants doesn't mean it's not important. Popularity != importance
        * Set (versus list)
        * How do we keep the list up-to-date
    * Probably should make changes to the tool so that it’s much easier to vary weights & see their impact across many projects. That’ll enable experimentation.
    * Amir didn’t want weighted scores - instead, just get the specific values instead of the scoring. E.g., “number of contributors”.
    * Next steps:
        * Criticality score will add ability to run on a single repo
        * Then can be integrated into ingestion automation
        * PoC is blocked until then.
    * The process described https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit
        * “The group works by consensus most of the time.” - this seems to be getting misunderstood. The purpose was to not argue about proposals that everyone agreed to (e.g., Linux kernel).
    * Process blocked waiting for a tool to be built.
        * Randall building, Caleb contributing tool.
        * However, this is starting to block the whole process.
        * Randall: I’d built stuff so you could load scripts. But it’s currently just npm. We could add, but then it’d be more work, & why not use criticality score for the data anyway.
        * David: Why not use Google Spreadsheets?
    * How big is the set of critical projects? How does something leave the set?
        * Rough consensus on maintaining a list of approx 100 (although not a hard limit)
        * Goal is for focus and funding and security audit efforts on especially important, and especially those at risk.
        * Current set doesn't taking into account riskiness at the moment
        * Discussion around the projects on the current set of critical projects and why they are there.
        * Do projects that have received enough funding/investment get removed?
        * Is this a list of importance, or important projects that have security problems?
            * David: the former. Projects go off the list when increasingly they aren’t being used (e.g., because other projects stop using it as a dependency, people have switched to something else, or no longer use that kind of functionality).
        * We should keep revisiting the list, and that will cause things to leave the list.

<h2></h2>


Attendees:



* Jeff Mendoza (Google)
* Jacques Chester (Shopify)
* Marco Benatto (Red Hat)
* Randall T. Vasquez (Gentoo)
* Antony Faris (MTRHSHP)
* Matt Rutkowski (IBM)
* Jeff Borek (IBM)

Regrets



* Amir:  Feel Better!

Agenda:



* New faces
    * Jeff Borek: been a while, welcome back
* [Jeff] Alternating time not happening? Will follow up with staff
* Charter:
    * Not enough for quorum
* Question about quarterly idea:
    * Easier to ingest for large orgs
    * Want to discuss with Caleb about criticality score as well
    * “**To Do**: Focused session on Ingestion Engine and Process for Identifying Critical Projects”
    * Planning on focusing in next meeting
* Should we put ingestion engine project/code in ossf org and link to readme?
    * Yes, AI: Jeff

<h2>Sep 8, 2022</h2>


Attendees:



* Jacques Chester (Shopify)
* Henri Yandell (AWS/Apache)
* Jono Spring (CERT/CC)
* Randall T. Vasquez (Gentoo)
* Peter Singh (Astro)
* Eric Tice (Wipro)
* Jonathan Leitschuh (Dan Kaminsky Fellowship for HUMAN Security)
* Jay White (Microsoft)
* Munawar Hafiz (OpenRefactory)
* Chris Ferris (IBM)
* Matt Rutkowski (IBM)

Agenda:



* New faces
* For next meeting: Discuss any feedback and vote/finalize/adopt charter
    * Voice vote - followed by approvals on the PR
* Munawar - volunteered to organize a virtual workshop “conference” for open source maintainers 
    * Hear about their problems, pain points, etc 
    * A way to engage top open source maintainers 
    * Coordinate with other working groups 
    * Important: Scope - what are the critical projects in which to reach out to maintainers 
    * Going to do the event more than once 
    * Different than office hours - virtual - 
    * About how many maintainers?
        * Potentially break down into smaller group sessions
            * Python
            * Java
            * Etc
        * Reconvene with larger group 
    * Small groups
        * 20-30 
        * About 4 - so about 100 total 
    * Munawar would like feedback! Email him with ideas: [munawar@openrefactory.com](mailto:munawar@openrefactory.com)
    * Idea: Small thank you for participating to maintainers/contributors - gift card? 
    * Randall - look into working with Best Practices group 
* Ingestion Engine
    * Randall: they’ve been working with criticality score project 
        * Lots of alignment and common goals
        * Finding ways to collaborate with them 
        * Right now exclusively using npm for ease of use 
* Discussion regarding identifying critical projects
    * Jacques: do a quarterly release possibly? Here’s the spring 2022 list of projects
    * Chris F: It’s important to think about who is using this and what the purpose of this list is 
    * Top projects will likely stay the same, shifts can happen 
    * Reasoning for Set: Provide a best-effort analysis of quantitative and qualitative research in the open-source ecosystem and generate a curated, living set of projects deemed “critical” to organizations & individuals. We say “set”, not “list”, because we’re not currently trying to order the entries inside the set.
* **To Do**: Focused session on Ingestion Engine and Process for Identifying Critical Projects

<h2>Aug 25,2022</h2>


Attendees:



* Jeff Mendoza (Google)
* Jacques Chester (Shopify)
* Caleb Brown (Google)
* Mihai Maruseac(Google)
* Randall T. Vasquez (Gentoo)
* David Edelsohn (IBM)
* Jay White (Microsoft)
* Munawar Hafiz (OpenRefactory)
* 

Agenda:



* New faces
    1. Welcome Katherine! 
* Workgroup gave an update to OpenSSF TAC [Jeff]
    2. Namely what we’re working on
        * i.e.  Phase I and Ingestion Form
        * See form(s) above
    3. Recent audit results
    4. Meeting recording [https://www.youtube.com/watch?v=rUNg7yOtPxg](https://www.youtube.com/watch?v=rUNg7yOtPxg)  ([https://www.youtube.com/c/OpenSSF/videos](https://www.youtube.com/c/OpenSSF/videos)) 
* Criticality Score project updates [Caleb]
    5. June data: [https://storage.googleapis.com/ossf-criticality-score/index.html?prefix=2022-06-07/](https://storage.googleapis.com/ossf-criticality-score/index.html?prefix=2022-06-07/)
    6. Productionisation Milestone 2 design: [https://github.com/ossf/criticality_score/pull/175](https://github.com/ossf/criticality_score/pull/175)
        * GitHub App has higher API limits vs Personal Access Tokens
    7. Updates on signals
        * Improving signals 
            * Some are not particularly helpful (org count, contributor count) 
        * Improving sources of data
* Charter Adoption feedback - [https://github.com/ossf/wg-securing-critical-projects/pull/58](https://github.com/ossf/wg-securing-critical-projects/pull/58)
    8. Feedback welcome
    9. Vote this session or next session? 
    10. Regarding Maintainers, etc
        * `  - iv. Maintainers are the initial Collaborators defined at the creation of the Technical Initiative. The Maintainers will determine the process for selecting future Maintainers. A Maintainer may be removed by two-thirds approval of the other existing Maintainers, or a majority of the other existing Collaborators.`
        * Workgroup to take vote on 9/8/2022
* Ingestion Form Feedback
    11. For reference: [Ingestion Form](https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps/prefill)
    12. May need links to other places ex: github and package manager
    13. Criteria may need to be checkboxes vs radio to indicate a combination of factors
    14. Should there be a freeform text field?
        * We do want the submission to be lightweight
* Talked about using github to host ingestion form and set
    15. On track thanks to Randall
    16. Will spend time at next meeting 9/8/2022 discussing this
    17. [jeff]: Should we setup an openssf repo for this?
        * [amir] possibly , let’s discuss. It can either live in the securing critical projects repo or be its own. 

<h2>Aug 11,2022</h2>


Attendees:



* David Stewart (Intel)
* Mihai Maruseac (Google)
* Jonathan Spring (CERT/CC)
* Jeff Mendoza (Google)
* Jay White (Microsoft)
* Matt Rutkowski (IBM)
* Randall T. Vasquez (Gentoo/Homebrew)
* Jacques Chester (Shopify)
* Amir Montazery (ostif.org) 
* Peter Singh [Fuzzy] (Astro Tech)
* Marco Benatto (Red Hat)
* Munawir Hafiz (OpenRefactory)
* Manu Magalhães (Sky UK)
* Álvaro Figueroa (Microsoft)
* Eric Tice (Wipro)
* David Edelsohn (IBM/GCC)

Could not join today (but sends his best)



* David A. Wheeler

Agenda:



* New faces
18. Welcome Marco, Manu, and Fuzzy!
* APAC friendly meeting time poll results
19. 8am Sydney, 3pm SFO, 6pm NYC, 10pm GMT, 11pm London
20. Post DST: 7am Sydney, 12pm SFO, 3pm NYC, 8pm GMT, 8pm London
21. Set next meeting (8/25) for new time: 8am Sydney, 3pm SFO, 6pm NYC, 10pm GMT, 11pm London
* Possible GitHub Demo [Randall]
22. PR a project, automatically pulls the criticality score 
23. Add Download count?
24. Anything else that would be in the pipeline
25. Make it templatized 
    * Consistent
    * Discuss ingestion engine form below
26. Question: should the PRs be in markdown or some kind of yaml
    * Markdown can be previewed
    * Yaml is more structured
    * WG consensus: yaml
* Discuss Ingestion Engine Form 
27. Link: [https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps/prefill](https://docs.google.com/forms/d/1AyBRzDXeFSJlHV-jtUPT1YIAq8rC9n34im_AUfIWfps/prefill)
28. Categorize answers - dropdown 
29. List of optional questions
    * How would you categorize it
    * Maintainer Community? 
        * Solo maintainer
        * Team
        * Org/foundation support
    * Package type (npm, pypi, etc.) Package link
    * Reasoning (crit score, etc.) should be a dropdown / checkbox list, with “other” for freeform
    * 
* CNCF/OSTIF Audit Impact Report [Amir]
    30. Link: [https://ostif.org/the-cloud-native-computing-foundation-and-ostif-impact-report/](https://ostif.org/the-cloud-native-computing-foundation-and-ostif-impact-report/)

<h2>Jul 28,2022</h2>


Attendees:



* Eric Tice (Wipro)
* Álvaro Figueroa (Microsoft)
* Randall T. Vasquez (Gentoo/Homebrew)
* Rahul Gupta (Microsoft)
* Matt Rutkowski (IBM)
* Yakov Manshin (Individual Developer / Researcher)
* Naveen Srinviasan
* Christine Abernathy (F5)

Agenda:



* New faces - Welcome Alvaro! Welcome Christine as well!
* Please fill out this poll to help determine the time we pick for the APAC friendly meeting time: [https://forms.gle/Toy3k9g8HYgN3TLj6](https://forms.gle/Toy3k9g8HYgN3TLj6) 
    31. APAC friendly time
    32. Please fill out form!
* Discuss using github as platform for Identifying critical projects (Randall) 
    33. Homebrew got involved (analytics pre-deployed)
    34. Started containerizing everything - putting a repo together
    35. Tools to run on github actions
    36. Envisioned: make a PR and add a name to the list
        * Trigger some checks and tools will create output
        * Measuring popularity, working with criticality score team
        * Can add more checks (can be suggestions) 
        * Add directly on github 
        * Will ensure not being exclusive (not too github focused) 
    37. Process brainstorm: Single pipeline
    38. Space to nominate project(s) 
        * Name of project
        * Github link
    39. Category (optional) 
        * Frameworks, Languages, etc
    40. Rationale or Selection Criteria
        * Have a menu of common selection criteria
        * Auto-generate and list the criticality score
        * Room for “Other”:
    41. Note: Have a link to the list in case it has been nominated already (maybe an autocheck?) 
    42. Output can be a markdown table 
        * Smart way of structuring 
        * Presented well to be analyzed
* Template for Ingestion Engine Form (Amir)
    43. Create some basic templates and data points
    44. Criteria for the list
        * Not updated in a year? Abandoned? 
* 

<h2></h2>


Attendees:



* David A. Wheeler (Linux Foundation) - apologies, must leave early
* Amir Montazery - leading today (Jeff Mendoza is having audio problems today)
* Jeff Mendoza
*  Mihai Maruseac (Google)
* Jacques Chester (Shopify)
* David Edelsohn (IBM/GCC)
* Chris Ferris (IBM)
* Henri Yandel (AWS/Apache)
* Matt Rutkowski (IBM)
* Randall T. Vasquez (Gentoo/Homebrew)

Agenda:



* New faces (Welcome new friends!)
    * Mihai Maruseac (not really new!) - joined supply chain security group, esp. SBOMs
* DCOs:
    * From David: “The TAC agreed today (2022-07-12) to start enforcing DCOs, after first alerting the WG leads and giving them time to provide feedback (that way the TAC can be alerted about any unexpected problems).”
    * TAC working on official wording, expect soon.
    * Plan to turn on GitHub status check for PRs in “ossf” org.
        * Will we use this app? [https://github.com/apps/dco](https://github.com/apps/dco)  - yes, that’s the plan. We’ll also enable a flag on GitHub.
        * More info: [https://github.com/ossf/tac/pull/115](https://github.com/ossf/tac/pull/115)
* OSS Europe ([https://events.linuxfoundation.org/open-source-summit-europe/](https://events.linuxfoundation.org/open-source-summit-europe/)) should have an OpenSSF day as well. Should have a CFP but haven’t heard yet.
* “Add Leads and contributors to projects/sigs” [https://github.com/ossf/wg-securing-critical-projects/pull/56](https://github.com/ossf/wg-securing-critical-projects/pull/56) 
    * Ok to merge?
    * David: Yes, but let’s try to convince Caleb Brown to lead package-feeds, at least for now. It’s much easier to “hand off” leadership if there’s a leader to start with, and package-analysis won’t work without package-feeds
* Work on Process Doc - [https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit](https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit) 
    * Agreement on reasoning/explanation for list? 
        * **Reasoning for Set: **Provide a best-effort analysis of quantitative and qualitative research in the open-source ecosystem and generate a curated, living setlist of projects deemed “critical” to organizations & individuals. We say “set”, not “list”, because we’re not currently trying to order the entries inside the set.
    * Ingestion Engine
        * The community can suggest/nominate a project.
1. Google Form - Standardized and agreed upon template
    1. Selection Criteria/supporting evidence for nomination
2. Feeds automatically into “Identified Projects”  - A Google Sheet
    * Google Form: Single point of entry - 
        * Single pipeline
        * Space to nominate project(s) 
            * Name of project
            * Github link
        * Category (optional) 
            * Frameworks, Languages, etc
        * Rationale or Selection Criteria
            * Have a menu of common selection criteria
            * Room for “Other”:
        * Note: Have a link to the list in case it has been nominated already (maybe an autocheck?) 
1. Action Item: For next meeting (7/28) - Amir will have a draft template done
    * Identified Projects Sheet: 
* Look into using Github as a platform
    * Randall is going to do initial analysis and discuss with the workgroup 7/28

<h2></h2>


Attendees:



* David A. Wheeler (Linux Foundation)
* julia ferraioli (Cisco)
* Jacques Chester (Shopify)
* Amir Montazery (ostif)
* Michael Scovetta (Microsoft)
* Jay White (Microsoft)
* David Edelsohn (GCC/IBM)
* Mihai Maruseac (Google)
* Henri Yandell (AWS, Apache)
* Chapman Pendery (Bloomberg/Syft/Gripe)
* Chris Ferris (IBM)
* Vinod Anandan (Citi)

Apologies:



* Hopefully not you!

Agenda:



* Hidden talents day
* New faces (Welcome new friends!)
    * Chapman Pendery (Bloomberg) - also work on Syft & Gripe
* OpenSSF day recap
    * Heard lots of great talks, good to meet people
    * Videos
        * Will be available to YouTube soon.
        * (Currently available through the conference website if you’re logged in.)
    * David W: Jacques gave a GREAT presentation on how to select critical projects (walking through alternatives, discussing pros & cons) - we’ll see more later in this meeting.
* [Jacques] [SEER project proposal](https://docs.google.com/document/d/1fuzzH01qjdGweZGNSziHJlFqXNB8p9me5CDXDGdxuf4/edit#heading=h.q80r2j54uy3y)
    * Background/justification for this approach is captured in [”How Do We Rank Project Risk?” by Jacques Chester](https://static.sched.com/hosted_files/ossna2022/a8/Jacques%20Chester%20-%20How%20Do%20We%20Rank%20Project%20Risk%20-%20SupplyChainSecurityCon%202022.pdf) from the Open Source Summit 2022 SupplyChainSecurityCon track
    * Issue: Do we reveal who the experts are?
        * David W: I think it’d be helpful to identify the experts, at least by pseudonym, is helpful, though not so critical. Don’t release the individual scores to the public.
        * Julia: Concerned about “painting targets on their backs”
        * Michael Scovetta: Make the list of experts public, not their individual results.
        * Julia: Need to consider unintended consequences of revealing list of names of experts.
        * Jacques: Could make name list opt-in. Could also assign all pseudonyms.
        * Julia: Good compromise would be to ID industry + area of experience.
        * Can post about the pool of experts.
    * Issue: Do we reveal the final results?
        * David W: I think this should be public. Others need to know what we’re focusing on, so they can focus on critical things they depend on but we aren’t. It’s also hard to hide what we’re focusing on.
        * Amir: It’s hard to hide what’s widely important anyway.
    * Why need experts?
        * Data is woefully incomplete. E.g., language dependency graphs won’t notice:
            * David W: Linux kernel
            * Julia: Development tools
        * “Can we [really] compare Linux kernel to Google Guava?”
            * Trying to create uniform sorting?
            * Yes, trying to create a single ranking
    * Related paper: [https://www.cs.utexas.edu/~ml/papers/semi-icml-04.pdf](https://www.cs.utexas.edu/~ml/papers/semi-icml-04.pdf)
    * Amir: I really want to have something we can iterate on soon. Current plan: [https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit](https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit)
    * Jacques: I think we should use the current plan for now, and then switch to this SEER process to acquire more expert opinions better. SEER will take time we don’t have.
    * Julia: Hard to capture economic data. Need to compare likish to likish, but in the end we need one list.
    * David: Separate “value” from “cost”. We can’t improve everything, we have limited resources, so we need to find a way to identify what needs resources first (so we focus our limited resources).
    * Chris Ferris: A list is in the eye of the beholder.
    * Need a “reason for the list” - what are we trying to do & why?
        * Added section  to doc: [https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g](https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g)
    * [https://theoryof.predictable.software/articles/some-requirements-for-a-universal-asset-graph/](https://theoryof.predictable.software/articles/some-requirements-for-a-universal-asset-graph/)
    * [https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit](https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit)
    * SEER can’t be ready to create a critical list by end of summer, so we’ll need to do the currenty-proposed (more ad hoc) approach for version 2 (we have an existing version, call that version 1). But if we want to use SEER in the future, we need to start implementing it now so it will be ready, so we need to raise it to the TAC.
    * 
* Project/SIG/Related updates - need to do next time
    * Criticality Score
    * Package Analysis
    * Package Feeds
    * OSTIF
    * Allstar
    * Identifying Critical Projects
* 

<h2></h2>


Attendees:



* You!
* Amir Montazery (ostif)
* Jacques Chester (Shopify)
* Jeff Mendoza (Google)
* Cameron Banowsky (SHE BASH)
* Jay White (Microsoft)
* Jonathan Spring (CERT/CC)
* David Edelsohn (IBM,GCC)
* Matt Rutkowski (IBM)
* Arnaud J Le Hors (IBM)
* Chris Ferris (IBM)
* Jonathan Leitschuh (Dan Kaminsky Fellowship @ HUMAN Security)

Agenda:



* New faces
    * Welcome Cameron! SHE BASH
        * Company contracted with federal government
    * Welcome Jay! 
        * Microsoft open source team
        * Standardize and streamline governance activities - easily accessible
* (Amir) Revisit charter discussion
    * Example charter: [https://github.com/ossf/wg-securing-software-repos/blob/main/CHARTER.md](https://github.com/ossf/wg-securing-software-repos/blob/main/CHARTER.md) 
    * Request is currently on pause - TAC is working on governance structure. 
    * Encouraged to work on: 
        * What is our scope - what is the scope of our working group 
        * We have draft goals in README.md (2 items) 
            * Can expand 
        * Think about “roles” 
            * Contributors, etc
            * SIGS/Projects
* Project updates:
    * Criticality Score - None
        * How does deps.dev fit into criticality?
    * Package Analysis - None
    * Package Feeds - None
        * AI: Jeff will see who is working on this and if they will continue as a part of this WG
    * OSTIF: Looking at cri-o, also fixed containerd. Incorporating project into ossfuzz.
        * Supplychain assessment as well as code review, roadmap to increase SLSA compliance.
        * More to come
    * Identifying Critical proj below
    * Allstar
        * Some new orgs using – notably, GSA (US govt).
        * Some new features were contributed
        * New release process publishing containers so others can run their own instances
        * Maybe parallel to deps.dev? Wondering if there’s any collab there.
            * Is integrating Scorecard results
            * Could we get them to visit the WG?
* (Jeff) Live update SIG/Project leaders?
    * Updated github with SIG/project leaders 
    * PTAL: [https://github.com/ossf/wg-securing-critical-projects/pull/56](https://github.com/ossf/wg-securing-critical-projects/pull/56) 
* Identifying Critical Projects
    * Expert elicitation - where that fits in
        * Automated scoring systems can be helpful
        * Sometimes the data can be misleading
            * Need human input 
        * Position: directly elicit estimates on risk - 
        * Current aim is an accurate (but likely imprecise) estimate sooner rather than later
    * 
    * [Process for Identifying Critical Projects](https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit)
        * Planes that come back example - 
        * Center for Naval Analysis statistical error about reinforcement of bombers that returned: survivorship bias.
    * Administrative - 100? Or just call it “List of Open Source Projects Identified as Critical” 
        * Definition for what we mean by that
    * List of What we’re working on - maybe avoid using term “critical” too much. 
    * (FOR NEXT MEETING) Continue discussion of high level items regarding this work (process, what it is, the scope, etc) 

<h2></h2>


Attendees:



* You!
* Jeff Mendoza (Google)
* Jacques Chester (Shopify)
* Mihai Maruseac (Google)
* Eric Tice (Wipro)
* David Edelsohn (IBM,GCC)
* Jonathan Spring (CERT/CC)
* Arnaud J Le Hors (IBM)
* Jonathan Leitschuh (Dan Kaminsky Fellowship @ HUMAN Security)
* Michael Scovetta (Microsoft)
* Joe Sepi (IBM)
* Vinod Anandan (Citi)
* Henri Yandell (AWS/Apache)
* Matt Rutkowski (IBM)
* David A. Wheeler (Linux Foundation)
* Chris Ferris (IBM)
* Georg Kunz (Ericsson)

Apologies:



* Amir, conflict

Agenda:



* New faces
    * Mihai Maruseac newly working on Supply Chain Sec
    * 
* [Jeff] Meeting time updates from last meeting:
    * Alternate EMEA/APAC
    * Agreed to create alternating times (eventually). Will poll for a good alternate date/time. The alternative time will also be a consistent date/time. We won’t schedule it on top of some other OpenSSF group.
    * Next meeting will be at the normal time, this will happen in the future.
* [Jeff] “Key Documents” above, courtesy of Amir
    * Key Documents
    * [Process for Identifying Critical Projects](https://docs.google.com/document/d/1r0oKMNlCaMUrsHx2PkIEzrPj4L_67Ia3XLGfeLldf0g/edit)
    * Notes:
    * Jacques: Where does expert solicitation still fit in? (Has an upcoming talk at Supplychaincon)
    * Michael: different verticals are interested in different projects, it would be good to keep track of which source we are getting info from to correlate which verticals these are important to.
        * Good to have an expert from each vertical to make sure we’re not making terrible assumptions.
    * How do we vote on >100 items? Sounds hard.
        * David: I think we need to use a ranked voting system, asynchronous
    * David: I think we need to capture “why is this important or not”, from as many people & analysis reports as can provide that data, BEFORE voting so that voting is informed. Capture that data asynchronously
* We need to define the term “critical project”
    * Ex: “significant cost and cause significant impact on people and organizations.” should be expanded on
* Chris: what is the significance of 100? I care about what is critical to my organization
* David: We have Openssf initiatives that care about a single list - we have limited resources, need to focus them, thus need prioritization
* David: Risk is also important, but perhaps that’s a separate thing we can measure once we have a list. E.g., # of maintainers, does it have security processes in place (e.g., tools in CI, badging), etc.
* Chris: I see the value of a list, I don’t see the value of voting.
* Georg Kunz: I see having verticals with their own list
* Jeff: does criticality == risk, or criticality == impact ?
    * risk == likelihood x impact 
    * Likelihood == frequency, impact == magnitude
* David: US Department of Defense “Risk, Issue, and Opportunity Management Guide for Defense Acquisition Programs” (2017) says, “Risks are potential future events or conditions that may have a negative effect on achieving program objectives for cost, schedule, and performance. Risks are defined by (1) the probability (greater than 0, less than 1) of an undesired event or condition and (2) the consequences, impact, or severity of the undesired event, were it to occur.” [https://acqnotes.com/wp-content/uploads/2017/07/DoD-Risk-Issue-and-Opportunity-Management-Guide-Jan-2017.pdf](https://acqnotes.com/wp-content/uploads/2017/07/DoD-Risk-Issue-and-Opportunity-Management-Guide-Jan-2017.pdf)
* Current WG projects/SIGs: [https://github.com/ossf/wg-securing-critical-projects#current-wg-projects](https://github.com/ossf/wg-securing-critical-projects#current-wg-projects)
    * Need to identify leaders (maintainers)
    * From: Previous meeting
    * we need to create a formalized project/SIG to identify critical OSS projects (at least globally, though the process may be reusable by others). Step 1: Identify project. Amir will take lead. Caleb Brown, David A. Wheeler willing to help.
        * Action Item: Create Document and Share with workgroup [Amir]
        * Link Process Document to working group notes [Amir]
        * Plan intensive working group session to work on process/document [Amir]
    * Who else wants to be involved as maintainer/leader?
        * Jacques, 
        * Scovetta [contributor]
        * Georg Kunz (contributing)
* [Working Group Status Reports ](https://docs.google.com/document/d/1_2uRLlKDzDDespETSuyiBVv13lEsj709om56n9vjmgU)
* 

<h2></h2>


Attendees:



* You!
* Eric Tice (Wipro)
* Jeff Mendoza (Google)
* Matt Rutkowski (IBM)
* Caleb Brown (Google)
* David A. Wheeler (Linux Foundation)
* Amir Montazery (Open Source Technology Improvement Fund)
* Abhishek Arya (Google)
* Naveen Srinivasan
* Jonathan Leitschuh
* David Edelsohn (IBM, GCC)
* Chris Ferris (IBM)
* Yakov Manshin

Apologies:



* Jacques is at RailsConf

Agenda:



* New faces
    * No new faces today
* [Caleb]
    * Criticality Score update, see [Milestone 1 doc](https://github.com/ossf/criticality_score/blob/main/docs/design/milestone_1.md)
        * Previous issue: [https://github.com/ossf/criticality_score/issues/102](https://github.com/ossf/criticality_score/issues/102)
        * Milestone 1 doc covers first step
        * Enumeration -> Collection -> Data Format
        * Use deps.dev as a source for depdents
        * Switch to Go
        * Jon: usecase: CodeQL can find projects that have bad patterns. Would like something to tell if a project is critical before pursuing
        * Amir: supports quantitative result for criticality
        * Abhishek: How can we incorporate Harvard Census study results and combine?
            * Need to improve usability of tool so researchers can run it
        * Data format output from criticality score should enable cross referencing with other results (Census)
        * How will the tool incorporate misses? (leftpad)
            * Need to be able to investigate misses, and compare with census to analyze and detect
        * Current score is weighted towards activity, in future will weigh in multiple inputs, ex: dependency, risk level
        * Some dependencies are hard to figure out, ex: used much in closed source code
            * Census 2 did incorporate proprietary code.
        * Like to use criticality score in own enterprise
        * Chris Ferris: I want to be able to adjust the weighting, run dependencies against THAT. It’d be really good if organizations could redo the process to see what is important to THEM
        * David: Maybe we need to define the requirements for the updated “process for identifying critical projects” as a first step, so we can move on from just continuing to talk
        * Current text from README on project/sig:
<h3>
                        **Current WG projects**</h3>


* [Securing Critical Projects: List of Critical Open Source Projects, Components, and Frameworks](https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit) - current version
        * Jeff: Maybe we need to create a formalized project/SIG to identify critical OSS projects (at least globally, though the process may be reusable by others). Step 1: Identify project. Amir will take lead. Caleb Brown, David A. Wheeler willing to help.
            * Action Item: Create Document and Share with workgroup [Amir]
            * Link Process Document to working group notes [Amir]
            * Plan intensive working group session to work on process/document [Amir]
* Package Analysis update, see blog post:
    * [https://openssf.org/blog/2022/04/28/introducing-package-analysis-scanning-open-source-packages-for-malicious-behavior/](https://openssf.org/blog/2022/04/28/introducing-package-analysis-scanning-open-source-packages-for-malicious-behavior/)
    * Some people are trying to run it, which is great, we hope to get some public contributions
    * LOTS of people heard about package-analysis. The widely-popular “Security Now!” podcast pointed to this and the OpenSSF (they incorrectly said it was the “first” OpenSSF project): [https://twit.tv/shows/security-now/episodes/869?autostart=false](https://twit.tv/shows/security-now/episodes/869?autostart=false)
    * They want to do more dynamic analysis, want to integrate more with others
    * Is there data in BigQuery?
    * I’ll provide a link. [https://console.cloud.google.com/bigquery?d=packages&p=ossf-malware-analysis&t=analysis&page=table](https://console.cloud.google.com/bigquery?d=packages&p=ossf-malware-analysis&t=analysis&page=table)
    * Jonathan: If there was a great way to pull criticality data using a REST API (e.g., a big csv file), that’d be really helpful.
        * Agreed, we’ll do it.
* Remaining Project Updates
    * [Amir]
        * Managed Audit Program update
            * Proposed Managed Audit Program for 25 projects in July 2021
                * Google and OpenSSF contributed to fund ~10 of the projects
            * About 50% done with preliminary projects, results coming soon
            * Publishing results in Q2 2022
            * Updated list of projects to audit to 50
                * Larger pool of projects to go through 
                * Data point for aggregation
            * Validate new list and put proposals together
* [David] Open Source Software Security Summit II (Washington, DC) met last week
    * Discussed: Open Source Software Security Mobilization Plan, see [https://openssf.org/oss-security-mobilization-plan/](https://openssf.org/oss-security-mobilization-plan/)
    * 3 goals, divided further into 10 streams. Stream 7 is especially relevant, these seem relevant:
        * APPENDIX 2: Stream 2: Establish a Public, Vendor-Neutral, Objective-Metrics-Based Risk  Assessment Dashboard for the Top 10,000 (or More) OSS Components ...................................19
        * APPENDIX 5: Stream 5: Establish the OpenSSF  Open Source Security Incident Response Team .........................................................................30
        * APPENDIX 7: Stream 7: Conduct Third-Party Code Reviews (and Any Necessary  Remediation Work) of up to 200 of the Most-Critical OSS Components Once per Year. ............38
* [Jeff] Meeting time?
    * Alternate EMEA/APAC?
    * Agreed to create alternating times (eventually). Will poll for a good alternate date/time. The alternative time will also be a consistent date/time. We won’t schedule it on top of some other OpenSSF group.
    * Next meeting will be at the normal time, this will happen in the future.

<h2></h2>


Attendees



* You!
* Eric TIce (Wipro)
* Jacques Chester (Shopify)
* Emily Fox (Apple, CNCF TOC & STAG)
* Matt Rutkowski (IBM)
* VM Brasseur (Wipro)
* Harimohan Rajamohanan (Wipro)
* Naveen Srinivasan
* Alan Gonzalez (Plume)
* Henri Yandell (AWS & Apache)
* David Edelsohn (IBM, GNU Toolchain)
* John Naulty (Coinbase)
* Amir Montazery (open source technology improvement fund)
* Pawel Pawlak (F5) 
* Michael Scovetta (Microsoft)
* Tom Bedford (Bloomberg)

Agenda



* New faces
    * Harimohan from Wipro OSPO. Dev and soln architect. Interested in security metrics and other OpenSSF projects
    * Pawel serving on ONAP security subcommittee as chair, wants to learn more about the WG and possible cooperation
* May 19th meeting update - likely going to be alternate time (Amir)
    *  so that Caleb can present on Criticality Score updates
    * Should be notified multiple ways: slack, calendar
* TAC, Charter, governance, life, the universe and everything
    * Charter looks to be on hold pending further TAC guidance
* [Jacques] SEER prototype demo
    * A tool for ranking projects using expert input
* [Scovetta] Omega automated security reviews (prototype)
    * [https://github.com/ossf/security-reviews](https://github.com/ossf/security-reviews) 
    * Concept: Run Omega security tools suite, if results come back clean, then auto-publish a security review attesting to that.
        * Suite: CodeQL, Detect-Secrets, NodeJSSscan, Semgrep, OSS-Reproducible, and osv.dev.
        * Initial target: top 100 indirect dependencies + version from Harvard Census II.
        * Anecdotally, about half of scanned projects come back clean.
    * Thinking about determining project review automatically
        * Ultimately would belong in a SCITT ledger, but could at first present in a templatized form
        * This could help us see whether this would provide value
        * Screenshot of possible templatized output below
        * MS is running it against the top 100 npm modules from the Harvard study and so far about half of the ones it’s done have come back clean
    * Reactions?
        * Amir: Conceptually is a very good idea
        * MS: And Omega has a target of 10K, so if half of those come back clean then maybe could do 20K? Won’t know if you can’t see; this allows us to see
        * MS: Will be OSS and containerized, so anyone can run and verify/duplicate
        * Amir: Could convert this into some sort of metric?
            * MS: scorecards could ingest data from security reviews
            * MS: The dashboard doesn’t do analysis _today_ but future revs could but only want to expose info that shows an indicator of “no problem”. Don’t want to risk disclosing vulnerabilities.
        * Jacques: Rather like a vulnerability canary? “It’s good, it’s good, it’s good…”
            * MS: Yeah, like “done 87 of these top projects…but curiously not these 13”
            * Could publish on a lag, but doesn’t gain much
            * People can already scan “no problems” data and look for the holes
            * Good philosophical argument haven’t discussed enough: where’s the tradeoff? Where’s the line for reporting?
                * Example: lgtm.com
                * Amir: can assume the baddies will always be doing bad things
            * Naveen: oss-fuzz logs vulnerabilities but gives 90 days before publishing
                * MS: trivial to add a 90 cooling off period to security review
                * But oss-fuzz gives 90 days notice to fix something…for security review problems go down the Omega route but if “no problems” then what would he report…?
            * John Naulty: Would be good to make reports avail to github maintainers of projects for 90 days then make public?
                * VMB: A significant percentage of critical projects aren’t on github
            * Matt Rutkowski: For ASF, maintainer != project management committee
                * Latter is subset of former
                * Broadening security stuff to entire maintainer community might not be desireable.
    * MS will drop a PR with output of the tool
        * [https://github.com/ossf/security-reviews](https://github.com/ossf/security-reviews) 
        * Add comments there
    * Matt Rutkowski: For these ranking exercises, it’s important to have different views on the data
        * Ex: IoT space has a different idea of what’s critical
        * So would be helpful to gather information (categorize) based on area
        * Jacques: some expert systems calibrate based on how expert performs in $x domain
            * 

![image](https://github.com/ossf/wg-securing-critical-projects/assets/128058721/1ca54b49-1433-4faa-8f8d-56ae309da331)

<h2></h2>


Attendees



* VM Brasseur (Wipro)
* Jacques Chester (Shopify)
* Eric Tice (Wipro)
* Jeff Mendoza (Google)
* Emily Fox (Apple, CNCF TOC & STAG)
* David Edelsohn (IBM, GCC)
* David A. Wheeler (Linux Foundation)
* John Naulty (Coinbase)
* Aeva Black (Microsoft, OpenSSF TAC)
* Cheng Lee (Anaconda)
* Vinod Anandan (Citi)
* Matt Rutkowski (IBM)
* Jory Burson (Linux Foundation)
* Michael Scovetta (Microsoft)
* Alan Gonzalez (Plume)
* Andrey Khalyavin
* Hen (Amazon)

  
Agenda



* New Attendee Intros
    * Cheng Lee - principal engineer at Anaconda; repping the Conda ecosystem.
* Charter Update (Jeff) - [https://github.com/ossf/wg-securing-critical-projects/issues/52](https://github.com/ossf/wg-securing-critical-projects/issues/52)
    * [https://github.com/ossf/project-template/issues/4](https://github.com/ossf/project-template/issues/4)
    * TSC = TSC voting members are initially the Technical Initiative’s Maintainers.
    * Maintainers are the initial Collaborators defined at the creation of the Technical Initiative
        * Jeff and Amir to start out, then will add Collaborators and Contributors
    * Mission will be transcription of Goals in Readme [https://github.com/ossf/wg-securing-critical-projects#goals](https://github.com/ossf/wg-securing-critical-projects#goals) 
* Qualifying the critical projects list with Scorecard, Harvard Census, community diversity, and security mindset (Michael Winser, Michael Scovetta).
* Alpha-Omega brief update (David Wheeler)
    * [https://openjsf.org/blog/2022/04/18/open-source-security-foundation-openssf-selects-node-js-as-initial-project-to-improve-supply-chain-security/](https://openjsf.org/blog/2022/04/18/open-source-security-foundation-openssf-selects-node-js-as-initial-project-to-improve-supply-chain-security/) - node.js first project
        * All of the millions of packages managed by npm are obviously not in scope.
        * NPM project itself not in scope
        * Node has v8 as a dependency. V8 itself isn’t in scope, but how v8 is brought in is absolutely in scope.
        * 2 areas: modify processes to provide long-term security improvements, and deal with existing vulnerability reports (triage / fix / etc.)
        * Node the money count is 9 months.
        * Plan to announce a few more Alpha projects soon.
        * Near the end of the year we intend to evaluate to see how well it worked.
        * The fact that we’re engaging with node is public, what it fixes etc. will also be public. We want it to be public. The term sheets, etc., probably won’t be.
        * Emily: Sometimes the engagements are ignored in CNCF because there was an expectation of the project doing too much.
        * Aeva: We should try to publish the framing of how Alpha starts, don’t wait until things are done. Be supportive. Know that projects will have limited time. Aeva & Emily Fox have offered to help write this up.
        * Emily: Last question I’ll post in chat to give others the floor - is the engagement expected to provide Node and other projects a structure on their responsibilities for reporting vulnerabilities?  There are occasions where we’ve seen projects purposely not file vulnerabilities or CVEs.  I can see the Alpha activities spawning an Open Source Vulnerability Management Promise.
            * Yes, part of the process work (understanding, suggestion improvements, helping to implement then) would include vulnerability disclosure.
        * Node wants to publish 1-2/month a transparency report, “here is what we’re doing, how we’re proceeding on our goals”
    * Currently interviewing for positions
* Securing critical toolchains (didn’t see Jenkins on the list) (Michael Winser, Michael Scovetta)
* Relationship between Alpha-Omega and Securing Critical Projects choice of critical projects
    * Timeline?
    * Should we be categorizing different tech stacks?: web startup vs industrial control for example
    * Aggregate prevalence of project and scorecard metrics - for which project would a small investment do most good.
    * Already mature projects don’t have easy security gains
    * Consider historical vuln counts (this can bias against well behaved projects)
    * On metrification: beware of [Goodhart’s law](https://en.wikipedia.org/wiki/Goodhart%27s_law)
    * 
  

<h2>April 7, 2022</h2>


Attendees:



* David A. Wheeler (Linux Foundation)
* Jacques Chester (Shopify)
* Jeff Mendoza (Google) [led meeting]
* Marina Moore (NYU)
* David Edelsohn (GCC, IBM)
* Eric Tice (Wipro)
* Jonathan Spring (CERT/CC)
* Aditya Sirish (NYU)
* Arnaud Le Hors (IBM)
* Darcy Clarke (GitHub/npm)
* Matt Rutkowski (IBM)
* Jeffrey Borek (IBM)

Agenda:



* New Attendee Intros
    * Marina (NYU) - have been in repositories WG, academic. Wants to know if there’s interest having a central place to store data about packages. If so, would this be the right WG? E.g.: libraries.io.
    * Arnaud - returning superstar
    * Jonathan Spring - CERTCC at CMU. Management and prioritization focus. Criticality looks related to prioritizing vulns. On CVSS and EPSS SIGs.
    * Darcy Clarke - npm CLI team. 
* Charter - [https://github.com/ossf/wg-securing-critical-projects/issues/52](https://github.com/ossf/wg-securing-critical-projects/issues/52)
    * Mostly boilerplate
    * Need list of maintainers: process for change
    * There is no “TSC”, and the TAC is elected, so it needs fixing for that. Remove many things
    * David: Focus on making the scope of WG clear.
    * We could have the scope in the WG README, and have charter point to that.
    * Brian: Let’s work on simple, I know some folks are working on the charter update.
    * Wait a bit for the charter text to get improved?
    * Agree on the leads (co-chairs), if there’s any process to be a member, we can make a more complex one later once things are fleshed out
* Process for creating an updated list of critical projects?
    * We created a draft list of about 100 critical projects quickly. Had to be done quickly to support a great MFA project. But we’d like to have a better process this time around, as well as using improved data.
    * Jacques is researching ways for humans to review data & make good (group) decisions, research still in progress
    * Jacques: in my presentation in February ([minutes](#heading=h.axfnk89cjoo1), [recording](https://youtu.be/8-w01-4CKqU?t=1489)), 2 things:
        * (1) voting system vs. elicitation of probabilities. Elicit probabilities from people instead of voting seems better according to the literature I’ve found. I’ve come up with a name for a prototype (Security Expert Elicitation of Risk = SEER).
        * (2) Need to build a prototype of what it’d look like for the expert.
    * David: How does this scale? Millions of OSS projects, hundreds from various ranking systems.
    * Jacques: That’s a challenge. Probability systems can be concrete for likelihood of vulnerability… but impact is way harder to measure. Yet impact is key part of risk measurement.
    * David: Commonality can hint at impact, though not the same thing.
    * We’ll need a lot of people to give a lot of impacts to handle the scale.
    * ?: There was a futures market for people who estimated security well, but people didn’t provide information even when offered money if they guessed well.
    * This is like thinly-traded stocks, just not enough data to work on.
    * David: Data like Harvard can at least point to specific projects, then humans can analyze it (so humans don’t have to figure out all the things to look at).
    * Most of the literature focuses on high-stakes with only a few decisions & few humans, we’re talking about a difference scale, literature doesn’t cover.
    * Sparsity of data is a big problem
    * Maybe we need to create rankings algorithmically, then use humans to adjust things
        * Can use popularity, e.g., https://popcon.debian.org/  and Harvard studies.
    * Next steps? Jacques will try to look further. Maybe we can prototype.
* Marina: discussion of proposal for a software repo data warehouse: [https://docs.google.com/document/d/1dxHxXgqy7t6e7IdNKa7Z0YJ2vkO0FTg7s_a-KNA5TEY/edit?usp=sharing](https://docs.google.com/document/d/1dxHxXgqy7t6e7IdNKa7Z0YJ2vkO0FTg7s_a-KNA5TEY/edit?usp=sharing) 
    * It’d be a lot easier to have a single DB to query
    * David W: There’s an existing system, libraries.io (Tidelift). I’ve contributed some patches to it in the past. Harvard & IDA used libraries.io to do dependency analysis. However libraries.io is not well-maintained. I think it WOULD be valuable to have something like this. There’s been some discussion about this with Harvard & LF Research.
    * Download counts, dependencies, etc.
    * Rubygems has the most details, including every request. PyPI has something similar. GitHub has their dependency graph (dependency-insights) works with maven, etc. - not clear it’s bulk avaiability
    * Marina: It’d be helpful to put this within the OpenSSF repository WG, but with collaboration with this critical projects WG

<h2>March 24, 2022</h2>


Attendees:



* VM Brasseur (Wipro)
* Jacques Chester (Shopify)
* Caleb Brown (Google)
* Chris Ferris (IBM)
* David A. Wheeler (Linux Foundation)
* Jeff Mendoza (Google)
* Amir Montazery (ostif.org)
* Jose Miguel Parrella (Microsoft)
* David Edelsohn (IBM, GCC)
* Alan Gonzalez (Plume)
* Henri Yandell (AWS/Apache)
* Joshua Padman (Red Hat)

Agenda:



* New Attendee Intros
    * Josh from Red Hat - product security
    * Alan from Plume
    * Jose Miguel Parrella (@bureado) - I work in the Office of the CTO, Microsoft Azure. First time here. Mostly learning, and excited about every bullet point in the notes doc :)
* Caleb - criticality score - deep dive proposal  [https://github.com/ossf/criticality_score/issues/102](https://github.com/ossf/criticality_score/issues/102) 
    * David W: Don’t make definitions depend on GitHub - many projects don’t use GitHub. GitLab, etc., are real! Just say “commits” or similar.
    * David W: Many small projects *don’t* need to be changed (e.g., isOdd) - probably shouldn’t consider “not edited a while” risk for projects of only a few lines of code (because it’s a noisy, dubious signal)
    * VM: Make sure rationale are documented (cite sources)
    * Jeff: Original criticality_score was simple and easy to understand (e.g. "GitHub busyness"). As it gets more complicated, how do we make sure it can be understood and trusted?
        * Suggestion: find some one further away from the problem (tech writer?) to help in this area
        * 
* Next meeting (April 7th) : Charter  - [https://github.com/ossf/wg-securing-critical-projects/issues/52](https://github.com/ossf/wg-securing-critical-projects/issues/52) 
* From “Future topic ideas” what are next steps for this?
    * Workgroup Github Page & gh issues #39-46 - +1
        * [https://github.com/ossf/wg-securing-critical-projects/issues](https://github.com/ossf/wg-securing-critical-projects/issues) 
    * Discuss gh issue #47 - ralight 
    * Overall, what should we do with suggestions for critical projects?
        * Jeff to come up with a proposal for storing suggestions until the next iteration of scoring.

<h2>March 10, 2022</h2>


Attendees:



* Matt Rutkowski (IBM)
* VM Brasseur (Wipro)
* Eric Tice (Wipro)
* Jeff Mendoza (Google)
* Jacques Chester (Shopify)
* Aditya Sirish (NYU, in-toto)
* Jon Meadows (Citi)
* Chris Lamb (Reproducible Builds)
* Chris Ferris (IBM)
* Yotam Perkal (Rezilion)
* Henri Yandell (AWS)
* Marta Rybczynska (OSTC)
* Jory Burson (Linux Foundation)
* Amir Montazery (OSTIF)
* Aeva Black (Microsoft)
* Appu Goundan
* Brian Behlendorf (Linux Foundation)
* Georg Kunz
* Jonathan Leitschuh
* Jeff Borek (IBM)
* Vinod Anandan (Citi)

Agenda:



* New Attendee Intros
    * Jonathan Leitschuh - Dan Kaminsky Fellow
    * Liran Tal - Snyk
* [Jeff] Quick announcement: March 24 meeting time
    * Caleb - criticality score - deep dive proposal  [https://github.com/ossf/criticality_score/issues/102](https://github.com/ossf/criticality_score/issues/102) 
    * One-off meeting 3/24
    * Proposed time: 8 AM AEDT (25th), 2 PM PDT, 5 PM EDT, 9 PM GMT
* [Harvard Census Release](https://www.coreinfrastructure.org/programs/census-program-ii/) - Open Discussion
    * Link to report: [Census II of Free and Open Source Software — Application Libraries](https://linuxfoundation.org/wp-content/uploads/LFResearch_Harvard_Census_II.pdf) 
    * Discussion on categories: 
        * Thoughts:
            * Time-based trend? 
            * Version specific seems to be more helpful
                * +1
                * Gives more detail into the packages
            * Filter by language?
                * Maven 
                * Can we potentially get a list chopped by language? – maybe
                    * Can ask Brian - or Frank 
            * Get a more consumable list? 
                * Something that can be “chopped up” 
            * Discussion with Frank: what are you thinking for Census III? 
                * What’s next? 
                * Getting data sets? 
                    * Analyze and Crunch that data 
            * The indirect 
        * Top 500 npm, Direct, Version Agnostic Packages
        * Top 500 Non-npm, Direct, Version Agnostic Packages
        * Top 500 npm, Indirect & Direct, Version Agnostic Packages
        * Top 500 Non-npm, Indirect & Direct, Version Agnostic Packages
        * Top 500 npm, Direct, Versioned Packages
        * Top 500 Non-npm, Direct, Versioned Packages
        * Top 500 npm, Indirect & Direct, Versioned Packages
        * Top 500 Non-npm, Indirect & Direct, Versioned Packages
            * Hen: Avalon-framework was shockingly high here. I think it’s due to Commons Logging (which is hard to know without domain knowledge). I’m going to raise the idea of dropping support for Avalon from Commons Logging.
    * Discussion: 
        * surprising - a few developers do a majority of development
            * I believe it was 136 developers – We should probably talk to these developers and see how we can help them
            * A “critical person dependency” - most development 
            * Lack of standardized naming 
        * Seems to be a consistent “80/20” distribution where a majority of work falls on a small number of developers
        * Shocking to see “avalon” on the list
            * Deprecated project - died in early 2000s
            * Worrying to see if its truly deprecated
        * Potentially identify projects that are deprecated - when was team last active? 
            * Issue a CVE? 
        * Problems getting CVEs from various CNAs
            * If not CVE at least some way to mark as “potential risk” 
            * CVE - lots of infra that can be leveraged
            * Open source centric CNA? - possibly - was done before but not currently afaik
* [Amir - will discuss at future meeting] Consensus-based Democracy - Constitution for Identifying and Curating a List of Critical Open Source Projects
* Open time for additional agenda items (can refer to Future topic ideas):
    * Outreach document
        * Check it out: [https://docs.google.com/document/d/1cO1SpGIfB-Du4tWrecT9z6E-1_JQsgxbtLvgm_9R_MU/edit#](https://docs.google.com/document/d/1cO1SpGIfB-Du4tWrecT9z6E-1_JQsgxbtLvgm_9R_MU/edit#)
* Github Issues
    * A few administrative ones can be closed 
        * Amir will take point on closing the administrative issues. Will discuss with the workgroup on the remaining ones
            * Follow-up: 3 issues were closed

<h2>February 24, 2022</h2>


Attendees:



* Jacques Chester (Shopify)
* VM Brasseur (Wipro)
* Eric Tice (Wipro)
* Matt Rutkowski (IBM)
* Aditya Sirish (NYU, in-toto)
* Chris Lamb (Reproducible Builds)
* Julia Ferraioli (Julia F is ok) (Twitter)
* David C Stewart (Intel)
* Georg Kunz (Ericsson)
* Brian Russell (Google)
* Henri Yandell (AWS)
* Vinod Anandan (Citi)
* Emily Fox (Apple, CNCF TOC)
* Yotam Perkal (Rezilion)

Agenda:



* [Julia Ferraioli] extended version of [outreach proposal](https://docs.google.com/document/d/1cO1SpGIfB-Du4tWrecT9z6E-1_JQsgxbtLvgm_9R_MU/edit#heading=h.72bzlhxyfdsp)
    * [https://docs.google.com/document/d/1cO1SpGIfB-Du4tWrecT9z6E-1_JQsgxbtLvgm_9R_MU/edit#](https://docs.google.com/document/d/1cO1SpGIfB-Du4tWrecT9z6E-1_JQsgxbtLvgm_9R_MU/edit#)
* Deep dive into Census II results - postponed for a future meeting
    * Webinar for the release is expected Tuesday, March 1st
    * Dedicated session set for 3/10/2022
* VM point - spdx experience
    * Inclusion principles - openly documented principles and criteria for inclusion 
    * Similar to “selection criteria” – need to be continually updated
    * SPDX - they have a public list, a living list 
        * Quarterly updated and released - for the end users (the published license list) 

<h2>February 10, 2022</h2>


Attendees:



* Josh Bressers (Anchore)
* David C Stewart (Intel)
* Julia Ferraioli (Twitter)
* Jacques Chester (Shopify)
* David A. Wheeler (Linux Foundation)
* Amir Montazery (Open Source Technology Improvement Fund)
* Jeff Mendoza (Google)
* Matt Rutkowski (IBM)
* Jon Meadows (Citi)
* Henri Yandell (Apache + AWS)
* Brian Russell (Google)
* VM Brasseur (Wipro)
* Eric Tice (Wipro)
* Melba Lopez (IBM)
* Chris Ferris (IBM)
* Caleb Brown (Google)
* Brian Russell (Google)
* Yotam Perkal (Rezilion)
* Aeva Black (Microsoft)
* Shubhra Khar (CTO of Linux Foundation, works on LFX)
* Brian Behlendorf (OpenSSF / LF)
* Vinod Anandan (Citi)

Agenda:



* (The WG meeting notes’ Google document settings didn’t allow world reading - we fixed that.)
* New Introductions:
    * VM “Vicky” Brasseur (Wipro)
    * Shubhra Khar
    * Caleb Brown
    * Chris Ferris
    * Melba (IBM)
    * Henri Yandell
    * Brian Russell
    * Yotam Perkal
* [David A. Wheeler] FYI: Harvard “Census II” report on application libraries has a final draft, it’s getting formatted/edited, and is expected to be publicly released on March 1.
    * Uses 3 SCA suppliers’ data, as well as dependency data from various repos (via at least libraries.io)
        * Vital to have 3 -> avoid concerns about trade secrets from SCA vendors
    * Tracks specific version #s (that’s different from the [preliminary version](https://www.coreinfrastructure.org/programs/census-program-ii/))
        * Big difference from the original version of the census
        * Naturally a lot more work, but figured out algorithms to get it done in a reasonable time
    * We should get ready to update or replace critical project list using this (& other) data
        * Heads up that we’ll need to figure out how to do this
    * Npm does things differently, which may complicate matters
        * Around half of all npm packages have 0-1 functions (0 function might export constants and/or just import other packages & things like that)
        * Very deep dependency trees, so can’t rely on number of dependencies in that situation
* [Jacques Chester] [Smooshing together expert opinions about project rankings](https://docs.google.com/presentation/d/117z1h4gM22yS7NlvORmcjCxjGKWvxWpn692LeV90J2g/edit#slide=id.g32426ed60e_0_59)
    * Presentation to workgroup (link to presentation: 
    * Ranking projects
        * Based on risk
        * Differentiating frequency and magnitude
    * Will continue discussion next week - need to get agreement on process for this next “v1” critical projects list, need to avoid debate forever but instead actually get it done.
* [Julia Ferraioli] ["Short" outreach proposal](https://docs.google.com/document/d/1cO1SpGIfB-Du4tWrecT9z6E-1_JQsgxbtLvgm_9R_MU/edit#)
    * Identify leads for each ecosystem - acts as a rep for that ecosystem/community
    * Important to consider method for finding representatives to avoid selection bias
* [Amir/Jeff] Brief discussion re: Town Hall update on Feb 23 – did not get to during meeting time - deferred to slack

<h2>January 27, 2022</h2>


Attendees:



* Jeff Mendoza (Google)
* David A. Wheeler (Linux Foundation)
* Emily Fox (Apple, CNCF Security TAG Co-Chair & KCCN Co-Chair)
* David C Stewart (Intel)
* Aeva Black [they/them] (Microsoft, OSI Board)
* Aditya Sirish (NYU, in-toto)
* Matt Rutkowski (IBM)
* Julia Ferraioli (Twitter)
* Amir Montazery (Open Source Tech Improvement Fund)
* Jon Meadows (Citi / CNCF Supply chain wg)
* Jacques Chester (Shopify)
* Tom Bedford (Bloomberg)
* Abhishek Arya (Google)

Agenda: (Led by Jeff Mendoza)



* New participants:
    * Jonathan (“Jon”) Meadows - runs a lot of supply chain work
    * Emily Fox - CNCF Security TAG Co-Chair & KCCN Co-Chair
    * Aeva Black
    * Tom Bedford
* Status of Census II from Harvard (Wheeler)
    * ETA mid-February
    * Once that’s released, expect this WG to go through it to decide what to add/change in the list of critical OSS projects
        * Workgroup Exercise - Analyze results of Census II 
    * NPM vs. not-npm
    * Direct vs. transitive
    * Discussion about how to merge results of this project with our own list
        * Suggestion to make process of including human input more robust - include input outside the group
        * 
* More general discussion about how to identify what’s critical & how
    * Julia: Want more experts involved to ensure representative viewpoints
    * Jacques Chester: Voting systems could help - but Ranking 200 is hard. Need to use a system that can handle partial order.
    * David: In the end humans need to make the decision (not just one human)
    * Could have software suggest 1 order, then humans could reorder (after human discussion)
    * How can we get more expertise involved?
    * Jacques: Could use range voting
    * David: Jacques, can you create a list of options on how to combine human information better & review next meeting? Julia, can you create a [short proposal](https://docs.google.com/document/d/1cO1SpGIfB-Du4tWrecT9z6E-1_JQsgxbtLvgm_9R_MU/edit#) on how to get more experts/expertise involved?
    * David: We should try to capture rationale on WHY it’s important, maybe create that as an input to voting.
    * Abhishek: We should continue the automation, e.g., criticality_score, esp for critical deps that big parts of ecosystem depend on (e.g. 8% of maven ecosystem depends on log4j, we can find similar projects easily, but criticality_score needs work)
* Alpha-Omega funded, in process of getting set up, will use this WG’s results as a starting point for its “Alpha” side
* Update on “Project List” and how it helped MFA project (Amir/David)
    * Question about next wave: Our WG should have a list at the quality of “1.0 release” before kicking off next MFA wave.
    * [https://github.com/ossf/great-mfa-project#](https://github.com/ossf/great-mfa-project#)
* Small update on criticality score (Jeff)
    * Aeva: I’d like to discuss this, but unlikely to make it to the end of the meeting. tl;dr; It appears the score is “upside down” w.r.t. projects like log4j - 
        * abhishek@google - please see comment above, this is something we are working on right now to find critical deps that everyone depends upon. Right now the criticality score is just a list of popular projects, but we plan to add the other aspect as well (we know this limitation and plan to fix it in Q1).
        * Wheeler: It’s currently  primarily a list of projects with lots of active commits/changes/contributors, not whether or not it’s popular (the Harvard study focuses on popularity but NOT on activity)
        * Aditya: @ Abhishek - what’s a good place to track the criticality score project?
            * [https://github.com/ossf/criticality_score](https://github.com/ossf/criticality_score) 
    * Caleb Brown will be working on this, timezone isn’t good for this meeting
        * Async communication is good, e.g., slack, issues, mailing list
        * We could have alternating times or a one-off meeting time
* Should we set aside “working time” in these meetings? (Jeff)
* Next meeting: How should we create the newer updated list of critical OSS?

<h2>January 13, 2022</h2>


Attendees:



* Amir Montazery (OSTIF)
* Jeff Mendoza (Google)
* Jacques Chester (Shopify)
* David Stewart (Intel)
* Julia Ferraioli (Twitter)
* Aditya Sirish (NYU, in-toto)
* Eric Tice (Wipro?)
* Bob Callaway (Google)
* 

Agenda:



* New participants:
    * Welcome Eric Tice. GSI. Team works with open source technologies. Devsecops. 
    * Welcome Julia Ferraioli. Twitter open source
* [Rubygems MFA RFC](https://github.com/rubygems/rfcs/pull/36)
    * Proposal that top Ruby gem owners are required to have MFA enabled.
* Criticality Score (Link to criticality score group notes: )
    * No new updates. They have their own meetings for up to date discussions. 
* Security Scorecards (link to their meeting notes and info here: ​​​​https://docs.google.com/document/d/1dB2U7_qZpNW96vtuoG7ShmgKXzIg6R5XT5Tc-0yz6kE
    * No new updates. They have their own meetings for up to date discussions. 
* Package Feeds
    * No new updates. They have their own meetings for up to date discussions. 
* OSTIF Managed Audit Program
    * Making progress on security engagements. 
* Identifying Critical Projects - Continue refining list of projects, discuss new additions/nominations
    * Updates have been made, including community contributions, such as: [https://github.com/ossf/wg-securing-critical-projects/issues/36](https://github.com/ossf/wg-securing-critical-projects/issues/36)
    * Beginning to look at prioritization of projects
    * 
* Allstar - see community calendar for talking about allstar in more detail 
    * Updates: determining project owner/representative aka repo administrator
    * Helps with finding abandonware. 
    * Allstar will fix your branch protection for repos out of spec
    * Planning for a performance upgrade
        * More robust architecture
* Tracking progress?
    * How did our identifying effort help the great mfa project? (https://github.com/ossf/great-mfa-project)
        * David can get us an update
    * Can we roll-up Allstar numbers?
        * Yes, we can pull install numbers and report, but where?

Lfx security: looking at data from lfx security – potential selection criteria

Using Security Scorecards to determine “water level” 

Workgroup Activity: Security Scorecards analysis 

Workgroup Discussion: worth finding critical projects? 



* Becomes relevant with proprietary tooling
    * They use old and outdated libraries for example
* General support, there is value in it

Good thing to think about: What “point” are we considering? Prod? Pre Prod? Came up during discussion of metasploit framework. 

Next thing: Categorization and expansion? Or continue adding projects? 



* Determine resources 
* Risk based approach

<h2>December 16, 2021</h2>


Attendees:



* Jeff Mendoza (Google)
* David A. Wheeler (Linux Foundation)
* Jacques Chester (Shopify)
* Aditya Sirish (NYU, in-toto)
* David Stewart (Intel)
* Tom Pollard (Codethink)
* Lior Kaplan (Checkmarx)
* Arnaud J Le Hors (IBM)

Agenda:



* New participants:
    * Arnaud Le Hors (IBM)
        * Feedback on critical project list
        * Open technology group
    * 
* Need to add logging utilities - log4j, tools identified by Census II preliminary
    * “Vulnerabilities in the Core: Preliminary Report and Census II  of Open Source Software” already had logback-core, slf4j:slf4j <github.com/qos-ch/slf4j>, and they’re on critical projects list
    * Adding log4j to critical list
* Prioritization discussion (Amir) - minor updates to list of projects
    * New column for status: done, no, or needs consideration
    * Suggestion to have separate tabs for approved projects vs needs consideration
    * Will need to eventually prioritize within approved projects for Alpha and other asks
        * Subsets (not stack rank)
        * Will need to be a combination of criticality (impact, how widespread it’s used) and likelihood (project health, accessibility from remote systems, etc.)
    * Risk = likelihood & impact. Size (larger more likely to have problem), activity (inactive may suggest unmaintained, esp. for a larger project).
    * [https://resources.sei.cmu.edu/asset_files/WhitePaper/2019_019_001_636391.pdf](https://resources.sei.cmu.edu/asset_files/WhitePaper/2019_019_001_636391.pdf) 
    * Wheeler: You always need humans. Census I ranked packages, but we found that you need humans to add context.
    * E.g., could use a ranked voting system to combine people’s insight, after tools provide information
* Propose doing some pri now?
    * 
* Github cleanup and update (thank you Jory!) 
    * Pull Request to update readme
* Github Cleanup and List Summary - Description 
    * Sheet feedback
        * Immediate info - up front info - also a guide for more information
        * Amir to take 1st crack at update
    * Github intro: 
* Critical project list for Alpha? (Jeff)
    * David to ask for fedback.
* Currently our goals are claimed to be:
    * Identify projects critical to the open-source supply chain.
    * Secure projects critical to the open-souce supply chain.
    * Provide tools and novel solutions for critical open-source projects
* Updated goal(s):
    * Identify critical open source software (OSS) projects.
    * Secure those projects.
* Critical OSS is software that, if vulnerable, subverted, or malicious, could cause serious widespread harm. This includes supply chain attacks, so OSS critical to OSS supply chains are included in critical OSS projects.
* No Meeting Dec 30th

<h2>December 2, 2021</h2>


Attendees:



* David A. Wheeler (Linux Foundation)
* Josh Bressers (Anchore)
* Amir Montazery (OSTIF)
* David Stewart (Intel) 
* Jeff Mendoza (Google)
* Jacques Chester (Shopify)
* Matt Jarvis ( Snyk )
* Lior Kaplan ( Checkmarx )

Agenda:



* New participants:
    * Josh Bressers 
* Complete by TODAY our first-cut identification of critical OSS projects (David A. Wheeler)
    * Great MFA Project needs this, [https://github.com/ossf/great-mfa-project](https://github.com/ossf/great-mfa-project)
        * Google tokens expire at the end of December, so we NEED a first cut NOW
    * Walk through sheet here: [https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit#gid=0](https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit#gid=0)
    * Start spreadsheet line 62, then jump to line 65
* First cut of ~100 projects identified as critical completed by the workgroup! Thank you to everyone who contributed!

<h2>November 18, 2021</h2>


Attendees: 



* David A. Wheeler (Linux Foundation)
* Jeff Mendoza (Google)
* Tom Pollard (Codethink)
* Amir (OSTIF)
* Lior Kaplan (Checkmarx)
* Jacques Chester (Shopify)
* Naveen Srinivasan
* Aditya Sirish (NYU / in-toto)

Jeff Mendoza & Amir are co-moderating.

Agenda:



* New participants:
    * Lior Kaplan
* How to identify some critical projects *soon* for the Great MFA Project (David Wheeler request)
    * [Great MFA Project](https://github.com/ossf/great-mfa-project) wants to distribute MFA tokens to critical projects
    * Need to identify critical projects. Originally mooted using Project Alpha list
    * Just learned yesterday (2021-11-17) that the Google-provided tokens expire at the end of the calendar year - not clear Project Alpha will have their list in time for coordination & distribution
    * Would like advice on starting point. Considering:
        * [OSTIF list](https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit#gid=0) (using data from Harvard, criticality score, underproduction, etc.) Count:86
            * Problem: some of these probably aren’t OSS projects. E.g., AMP is almost certainly not
            * We can do that right now
        * Top projects listed in Census II (use its preliminary list if nothing else) - the OSTIF list included entries from the preliminary list
        * Other info: [https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects.md](https://github.com/ossf/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects.md)
    * Would like other suggestions
    * ~1000 tokens to give away
    * Perhaps MFA Project can contact each project (not each person)
        * Naveen - open an issue in their github? That way it’s more public and transparent
    * For more projects: top 10 downloads for each language?
        * Ex: dotnet: [https://www.nuget.org/stats/packages](https://www.nuget.org/stats/packages) 
        * Npm: [https://gist.github.com/anvaka/8e8fa57c7ee1350e3491](https://gist.github.com/anvaka/8e8fa57c7ee1350e3491)
        * Rubygems provide daily data dumps: [https://rubygems.org/pages/data](https://rubygems.org/pages/data)
        * Pypi: [https://pypistats.org/top](https://pypistats.org/top)
    * All main languages and package managers
* Improve OSTIF list now - group reviewed the list, to determine if it made sense & add URLs
    * AMP project - there were concerns, what we mean is the GitHub projects (which sare OSS)
    * Looked at criticality score to verify that we have the right thing
    * Determination made for each project by workgroup (YES/NO/PENDING DISCUSSION)
* Open Floor for requests
* [Scovetta] With an Alpha-Omega announcement coming soon, I’d like to ask this WG to come up with a shortlist of ~10-15 “most critical” projects to go after. The list doesn’t need to be validated with the project maintainers themselves, but they should be ones that we’d feel good becoming an Alpha engagement. Could I request this list be completed in the next few weeks? (December 3?)

<h2>November 4, 2021</h2>


Agenda:



* Welcome and Introductions
    * Michael Winser - Google - focused on OSS Security
* Open Discussion and Updates: 
    * Jeff: Allstar update - 
        * OSSF Blog Post - lots of installs - https://openssf.org/blog/2021/08/11/introducing-the-allstar-github-app/
            1. Addressing feedback on GH
            2. Feedback: Improve Documentation
        * Lots of updates and development happening
    * Policies - things you can check on GH
        * Check MFA? 
            3. Including on package repositories
        * Ideas for policies
            4. Github Actions - files that can be read and analyzed
    * Jacques: Looking at supply chain security, specifically in ruby (but interested in node)
    * David Stewart: All-star vs In-toto - any overlap? 
        * They are complimentary 
        * In-toto: Snapshot of something in time, an attestation
        * In-toto: Attestation to an All-star run
            5. +1
        * Allstar gives insight into posture of project security-wise
            6. Wants to change settings too
    * Michael Scovetta: I normally have a conflict at this time, but re: Alpha, I wanted to make sure we were aligned on the process for choosing "most critical" OSS projects/systems. Is this something that this WG will own/drive? The Alpha project PM can drive the execution of that process, or if this WG wants to own it end-to-end and just deliver the prioritized list of projects, that'd be fine too. Just want to get clarity sooner rather than later.
        * Amir: the WG would help with identifying candidates. The WG can help in the identification of the most critical projects (including the why)
        * [https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit?usp=sharing)
        * Michael: from the WG the PM on the project would do their own selection process based on the state of the project (especially early phases).
            7. [Scovetta] Sounds great! We’ll have the Alpha PM regularly participate in this WG to stay aligned.
        * Spirit of executive empowerment to the Alpha-Omega team
        * Bias for action, especially early on.  +1 from Amir: We want to demonstrate progress and results
        * Proposed workflow
            8. Securing Critical Projects WG processes and analyzes input from multiple data points to generate list of “Candidate Projects” 
                * 
            9. Alpha “Core Team” pinpoints specific projects for engagements
                * The what/why/how
                * Based on state of the project
                * analysis/scoping required
            10. “Approval Board” consisting of GB/Lead/Project Reps approve engagements 
    * Chris Lamb
        * Misc event updates
            11. [Package con](https://packaging-con.org/)
        * Tar vulnerability [https://www.openwall.com/lists/oss-security/2021/10/03/1](https://www.openwall.com/lists/oss-security/2021/10/03/1) 
    * Andrey: [https://raw.githubusercontent.com/sslab-gatech/Rudra-Artifacts/master/paper/sosp21-paper341.pdf](https://st1.zoom.us/web_client/5g6glw/html/externalLinkPage.html?ref=https://raw.githubusercontent.com/sslab-gatech/Rudra-Artifacts/master/paper/sosp21-paper341.pdf)
    * Sergio
        * Academy Software Foundation meeting
        * Looking for presenters on security topics
        * [https://docs.google.com/document/d/1td8lt-mWWBTrnlpAH5Szra0GF6qisj0RYxzVlAAx1Qs/edit#heading=h.k0jwg34irfxy](https://docs.google.com/document/d/1td8lt-mWWBTrnlpAH5Szra0GF6qisj0RYxzVlAAx1Qs/edit#heading=h.k0jwg34irfxy)
        * “Also struggle with the discussion around security, what does it mean to deliver secure software, we follow the guidelines, but no one in OpenVDB claims to be an expert on the topic. Would be good to invest in education / expertise. Wave: identify what the projects would like advice / guidance, and then get someone to present on the topic, for instance Apple has a lot of security experts, people in VFX facility may not have a lot of exposure to.”
        * [https://openssf.org/training/courses/](https://openssf.org/training/courses/) -- free courses on secure open source development
* Jeff: Quick discussion about possible GitHub apps for preventing Sock-Puppet attacks
    * Which working group is best? This one ok, also think about best practices wg
    * Jeff will write up a description.

Attendees: 



* Jacques Chester (Shopify)
* Chris Lamb (Reproducible Builds)
* Tom Pollard (Codethink)
* Amir Montazery (Open Source Technology Improvement Fund, Inc)
* Jeff Mendoza (Google) 
* Michael Winser (Google) 
* Dave Stewart (Intel)
* Michael Scovetta (Microsoft -- online, not in meeting)

<h2>October 21, 2021</h2>


Agenda:



* Welcome and Introductions
* [Alpha-Omega: A Proposal for Improving Security Quality of Open Source at Scale](https://docs.google.com/document/d/1u7Ps18dzu9M-HF7ZHTK6VB5jLaVJvnw6uq3o7qw5yGE/edit?usp=sharing) [Michael Scovetta, OpenSSF (Microsoft)]
* Two asks for this WG
* What are the [100 most critical OSS projects](https://docs.google.com/spreadsheets/d/1YDd9tCDyBrzA6F_xmkx8atwT3Bam46f5IH33mmUM24A/edit?usp=sharing)?
12. Derek OSTIF to Everyone: We have an internal MAP250 doc that we can share with you Michael.

13. Important to have quantitative, but must have human overlook.
  * Criticality score.
  * Human curation (collaboration on google sheet?/voting or weighting mechanism?)
14. [https://docs.google.com/spreadsheets/d/1YDd9tCDyBrzA6F_xmkx8atwT3Bam46f5IH33mmUM24A/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1YDd9tCDyBrzA6F_xmkx8atwT3Bam46f5IH33mmUM24A/edit?usp=sharing)
  * Mixture of criticality score, form responses, # of contributors
    
15. OSTIF/Amalgamated list: [https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit#gid=0](https://docs.google.com/spreadsheets/d/1ONZ4qeMq8xmeCHX03lIgIYE4MEXVfVL6oj05lbuXTDM/edit#gid=0) 

16. Should also consider if the project is already well funded.
  * Use list to identify candidates, then discover more when investigating.

17. Propose all main package managers and repositories are included.
18.  
19. 

*When engaging with these projects, what should we be looking for?
20. Needs to be specific per project for Alpha
21. Present a menu of actions, decide what is appropriate
  * Need to figure out the menu
  
* How to measure improvement?
    * Scorecard improvement
    * CII Best Practices badge (got / improved score)
    * Threat model updated
    * Vulnerabilities found & fixed
    * Improvements in processes that reduce the likely number of future vulnerabilities
* Timeline
    * Hope to blog in Nov
* Time Permitting
* Workgroups Github Updates and Preliminary List of Identified Projects (Amir, ~15 mins)
    * [https://github.com/Amir-Montazery/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects.md](https://github.com/Amir-Montazery/wg-securing-critical-projects/blob/main/Initiatives/Identifying-Critical-Projects.md) 
* Very quick update regarding OpenSSF/OSTIF/php symfony (Amir,~5 mins)

Attendees: 



* Kim Lewandowski (Chainguard)
* Aditya Sirish (NYU, in-toto)
* David C Stewart (Intel)
* Chris Lamb (Reproducible Builds)
* Jacques Chester (Shopify)
* David A. Wheeler (Linux Foundation)
* Mike Malone (smallstep)
* Amir Montazery (Open Source Technology Improvement Fund)
* Jamie Magee (Microsoft)
* Tom Pollard (Codethink)
* Michael Scovetta (Microsoft)
* Jeff Mendoza (Google)

<h2>October 7, 2021</h2>


Agenda:



* Welcome and Introductions
* Updates to github page (Amir made some updates to run by the workgroup)
    * Start putting together info on critical projects
    * Update github with info on critical projects
* OSTIF
    * Software improvement model, improving security of 8 critical projects
    * Derek: currently 14 projects, 8 are funded by Google.
    * Generally not just a security audit, but working with OSS project to find & fix gaps - gives longevity to results
    * Problem: Hard to explain specific security concepts, e.g. what is fuzzing. 
        * Free course: on edX - recommend taking the course - [https://openssf.org/training/courses/](https://openssf.org/training/courses/)
        * [https://github.com/ossf/secure-sw-dev-fundamentals/blob/main/secure_software_development_fundamentals.md](https://github.com/ossf/secure-sw-dev-fundamentals/blob/main/secure_software_development_fundamentals.md) - actual content
* Status of Harvard research to id critical projects (have agreement on a third SCA dataset, but haven’t received it, reminders sent)
    * Quick update: they need 3 SCA datasets. They have 2, but haven’t received the data on the third one. Hasn’t happened yet. Being escalated to procure the data. Working on getting version numbers and the 3rd SCA dataset. 
    * Harvard research is adding tracking of version#s (based on David’s work)
* David A. Wheeler: Recommend adding “common package managers, & common repository infrastructure” to the set of critical projects - they’re the basis for distributing/updating most of the rest. E.g.:
    * (node.js) npm, npm repo
    * (Python) pip, PyPI
    * (Ruby) bundler, RubyGems
    * Don’t limit to language-level - containers & system-level matter too, may want to identify some of them as well
    * freedesktop-sdk runtime for flatpak
    * Some are well-funded, some are not. Create a list of them, & identify which ones need help & which ones need less help
    * Hardening the repos, adding detection for malicious, etc.
* Where do we store this info? Let’s start collecting a list of plausible sets of critical projects on our GitHub site. We can put a main list in the README, which links to the details. Amir will create a first draft on the WG’s README. Starting point:
    * “Open Source Software Projects Needing Security Investments” (aka Census I) (Wheeler & Khakimov, 2015) - older report, focuses on Linux packages [https://www.coreinfrastructure.org/wp-content/uploads/sites/6/2018/04/pub_ida_lf_cii_070915.pdf](https://www.coreinfrastructure.org/wp-content/uploads/sites/6/2018/04/pub_ida_lf_cii_070915.pdf)
    * “Core Infrastructure Initiative (CII) Open Source Software Census II  Strategy” (Wheeler & Dossett, 2017) [https://www.ida.org/-/media/feature/publications/c/co/core-infrastructure-initiative-cii-open-source-software-census-ii-strategy/d-8777.ashx](https://www.ida.org/-/media/feature/publications/c/co/core-infrastructure-initiative-cii-open-source-software-census-ii-strategy/d-8777.ashx) - discusses approaches for identifying critical OSS. Some key code from that is here (which handles version numbers in dependencies): [https://github.com/ossf/oss-analysis-census2-prototype](https://github.com/ossf/oss-analysis-census2-prototype)
    * “Vulnerabilities in the Core,’ a Preliminary Report and Census II of Open Source Software.” (aka Census II) (Harvard preliminary results) - focuses on language-level packages - [https://www.coreinfrastructure.org/programs/census-program-ii/](https://www.coreinfrastructure.org/programs/census-program-ii/)
    * U of Washington Underproduction study
    * OpenSSF Criticality Scores [https://github.com/ossf/criticality_score](https://github.com/ossf/criticality_score)
    * Package managers & repos - We need to do an analysis to identify which ones most need help. Possibly start by creating a list of these systems, e.g., [http://www.modulecounts.com/](http://www.modulecounts.com/)
    * OSTIF’s list of critical projects (Harvard + underproduction paper) [https://docs.google.com/spreadsheets/d/1oytKuD7UCX6nDXWQMr6ZgYYgap_SH_JVBof5gNrgSxo/edit#gid=0](https://docs.google.com/spreadsheets/d/1oytKuD7UCX6nDXWQMr6ZgYYgap_SH_JVBof5gNrgSxo/edit#gid=0)
    * SOS helped projects like [https://nixos.org/](https://nixos.org/) consider security improvements and looking into SLSA compliance standards.
* Is github a single point of failure for source?	
    * David: Valid, but lower risk than the other things we’re worrying about.
    * Scorecard main text allows any forge. They currently only implement GitHub due to limited time, they have to prioritize. Patches welcome!

Attendees: 



* Abhishek Arya (Google)
* Amir Montazery (OSTIF)
* Mike Lieberman (Citi/CNCF FSUG/CNCF Supply Chain WG)
* David A. Wheeler (Linux Foundation)
* Dave Stewart (Intel)
* Jeff Mendoza (Google)
* Naveen Srinivasan
* Derek Zimmer (OSTIF)
* John Naulty, Coinbase
* Tom Pollard (Codethink)
* Tim Miller (Citi)
* Appu Goundan (Google)

Regrets:



* Chris Lamb (Reproducible Builds; illness)

<h2>August 12, 2021</h2>


Attendees:



* David A. Wheeler (Linux Foundation)
* David C Stewart (Intel)
* Tom Pollard (Codethink)
* Mike Malone (smallstep)
* Michael Scovetta (Microsoft)
* Abhishek Arya (Google)
* Cristina Munoz (independent)
* Amir Montazery (Open Source Tech Improvement Fund)
* Jeff Mendoza (Google)
* Kim Lewandowski (Google)

Agenda:



* Discuss how to list critical OSS projects that have received funding (so others can see that it’s happened before & learn from it). Security Threats WG has a collection. CII has a page about this [https://www.coreinfrastructure.org/programs/audit-program/](https://www.coreinfrastructure.org/programs/audit-program/) ; David A. Wheeler is happy to edit that (e.g., to have its old page link to some updated OpenSSF information). What’s the best way to maintain this information?
    * we don’t want to remove this page as it’s historical, but we can point them over to openssf and what should we say?
* new discussion mailing list for malware detection/typosquatting
    * [https://groups.google.com/g/malware-detection-discuss](https://groups.google.com/g/malware-detection-discuss)
* PackageCon - [https://packaging-con.org/](https://packaging-con.org/) - proposals due Aug 31. Typosquatting work & detection malicious package seem relevant. Also “what’s critical” seems relevant. Actual conference 9 & 10 November 2021, appears to be virtual. Possible submitters:
    * PyOxidizer - Cristina
    * Kim will try to contact the typosquatting & detection.
    * David probably can’t, he plans to be in Paris
* Updating CII Audit page [https://www.coreinfrastructure.org/programs/audit-program/](https://www.coreinfrastructure.org/programs/audit-program/)
    * David A. Wheeler finally got the rights to edit that. He thinks that page should continue to exist (for historical purposes), but start with a link to the OpenSSF & this  group.
    * What should it say/cover? Suggestions?
* Allstar launch! 
    * blog post: [https://openssf.org/blog/2021/08/11/introducing-the-allstar-github-app/](https://openssf.org/blog/2021/08/11/introducing-the-allstar-github-app/)
    * It’s a GitHub app that enforces various security apps (e.g., branch protection, dependency checking, etc.)
        * It can check presence of files, contents of repo, settings of repo
        * Not sure it can check URL contents - should be able to, though hadn’t thought about that (e.g., it’s pursuing a CII Best Practices badge)
    * Project page: [https://github.com/ossf/allstar](https://github.com/ossf/allstar)
    * Looking for code contributors
* Secure Bioinformatics Reuse (Raymond “Ray” A. LeClair from In-Q-Tel Labs (iqt labs)) - presentation - ​​[https://drive.google.com/file/d/1mPUkz6t3Iq5yCtY1rZIVyfuqG9IKz_0J/view?usp=sharing](https://drive.google.com/file/d/1mPUkz6t3Iq5yCtY1rZIVyfuqG9IKz_0J/view?usp=sharing)
    * “Open source bioinformatics software packages”
    * “An initial security assessment of Open Source Bioinformatics Software”
    * Followed package security analysis framework created by Ruian Duan & coauthors, 2020, “Towards Measuring Supply Chain Attacks on Package Managers for Interpreted Languages” [http://arxiv.org/abs/2002.01139](http://arxiv.org/abs/2002.01139) . First cut:
        * Grep to search bioconda recipes / Dockerfiles for exfiltration commands
        * Using Aura to scan Python-based
    * “BioContainers” is a community-developed project that provides infrastructure & basic guidelines to create / manage / distribute bioinfomratics pacakges using Conda with Docker
    * Aura is a static analsysis framework to counter malicious packages & vulnerable code published on PyPI
    * Nextflow is a framework that lets users create data-intensive computational pipelines
    * Nf-core is curated set of analysis pipelines
    * Most popular genetic sequencing techniques are limited to at most 256 characters - this is a major limitation that requires tools to chop up genomes into many small pieces
    * Found a LOT of curl calls in conda installs; didn’t find anything suspicious
    * Scanned 495 bioinformatics repositories, used Aura to detect suspicious code
        * Aura produces LOTS of code; Aura Borialus tries to make it understandable
        * Didn’t find calls that appeared malicious
        * Did find SQL injection opportunities (usually that doesn’t contain user information)
        * Arbitrary Python code could be executed during install of Luigi & snakePipes (related to building workflows)
    * Any suggestions on how to identify problematic packages?
        * Dealing with false positives & massive data is a big problem, computationally heavy. Need more work there, reduce false+ and increase explainability
    * John Meyers: To id the critical packages in THIS area, contacted bioconda maintainers & companies focused on bio.
        * Still to do: contact clinical data companies, etc.
    * Amir: Did you consider doing third-party manual security reviews of any of these?
        * Good idea & would be a useful step, but wasn’t part of this project.
    * Wheeler: Some may be interested in the computer attack via DNA from 2017, where the malicious code was encoded in DNA & performed a buffer overflow attack (though it’s not really a practical attack today) - [https://techcrunch.com/2017/08/09/malicious-code-written-into-dna-infects-the-computer-that-reads-it/](https://techcrunch.com/2017/08/09/malicious-code-written-into-dna-infects-the-computer-that-reads-it/)
* Harvard research: Have data from 2 SCA suppliers, still working on #3
    * They’re also working to consider version numbers as part of dependency analysis
* Amir to chat about updated github page for this WG
    * [https://github.com/ossf/wg-securing-critical-projects](https://github.com/ossf/wg-securing-critical-projects)
    * Amir’s changes [https://github.com/Amir-Montazery/wg-securing-critical-projects](https://github.com/Amir-Montazery/wg-securing-critical-projects)
    * Please make a pull request -- Amir to make new pull request
        * See PR: https://github.com/ossf/wg-securing-critical-projects/pull/24
    * Wheeler: Should we add links like this?: [https://linuxfoundation.org/blog/funded-open-source-security-work-at-the-linux-foundation/](https://linuxfoundation.org/blog/funded-open-source-security-work-at-the-linux-foundation/)
        * Kim: Yes, don’t see why not
        * Amir: Agreed -- added to the Research and Publications folder in my edits. 
* The meetings will be monthly, it doesn’t make sense right now to do it every 2 weeks. We’re instead creating more focused meetings on specific topics.

<h2>July 1, 2021</h2>


Attendees



* Derek Zimmer, Open Source Technology Improvement Fund, Inc
* Amir Montazery, Open Source Technology Improvement Fund, Inc
* Daniel Silverstone (Codethink)
* Michael Scovetta (Microsoft)
* Kim Lewandowski (Google)
* David C Stewart (Intel)
* Aeva Black (Microsoft)

Agenda



* Current plan is to switch this meeting to monthly
* George to present results on a survey study of security and code reuse habits among 150+ software developers and data scientists [15-20 minutes]
    * Surveyed developers and data scientists for more security and reuse focused data
* [Kim] Security Scorecards V2! [Google blog post](https://security.googleblog.com/2021/07/measuring-security-risks-in-open-source.html)
    * ossf/scorecards
        * See the security checks it looks for
        * All (28k) metrics now integrated into [metrics.openssf.org](https://metrics.openssf.org).
        * Scorecard “Badge”
            * Something for projects to demonstrate they are have a high score
* SLSA just had its first WG meeting yesterday
* Security Assessment Update: Linux Kernel Release Signing Review
    * [https://ostif.org/a-review-of-the-linux-kernels-release-signing-and-key-management-policies/](https://ostif.org/a-review-of-the-linux-kernels-release-signing-and-key-management-policies/)

<h2>June 17, 2021</h2>


<h2>Canceled</h2>


<h2>June 3, 2021</h2>


Attendees



* David A. Wheeler, Linux Foundation <dwheeler@linuxfoundation.org>
* David C Stewart (Intel)
* Derek Zimmer (Open Source Tech Improvement Fund)
* Amir Montazery (Open Source Tech Improvement Fund)
* Tom Pollard (Codethink)
* Andrey K
* Jeff Mendoza (Google)
* John Myers
* Ed Baunton

Agenda



* Intros and welcome new folks!
    * No new folks in the meeting
* NIST Conference 6/2-6/3 updates
    * David A. Wheeler - presented
        * You can see the final drafts of what David submitted here: [https://docs.google.com/document/d/13SS6u2bQswfRYNi-WXm4dJZkgGHZK7slYZQ75HXTVns/edit](https://docs.google.com/document/d/13SS6u2bQswfRYNi-WXm4dJZkgGHZK7slYZQ75HXTVns/edit)
    * NIST had panel discussions looking at parts of the White House EO.
    * One panel was on identifying critical software:
        * Various Opinions
        * NIST appears to be struggling with this definition of critical software. 
        * Doing it in phases. - NIST needs a definition by 6/26
        * Challenges: going for a very broad definition, less focus on context
        * Falling into “category trap” where they identify categories but then say everything in there is critical. 
        * David Wheeler is stressing the risk-management approach. 
    * David Stewart: context is important, but reality is extremely important, especially when looking back at development. 
* Table-Top Exercise Results
* Updates on “Allstar” 
    * Working on getting ready for a production state. 
    * Get it to above “demo level” and be able to run it as a service and be reliable.
    * Getting to an openssf app in github
    * Try on a couple repos - beta test on a couple repos
        * Reach out: see if they’d be interested in testing
* Updates on Criticality Score
    * It is being used in analysis. John Meyers did some analysis using the criticality score tool.
        * Used the tool to identify the most critical python packages
        * John Meyers: We used the criticality score here. It was really helpful. [https://www.iqt.org/gitgeo-discover-the-geography-of-open-source-software/](https://www.iqt.org/gitgeo-discover-the-geography-of-open-source-software/)
* Updates on package-feeds
    * Tom Pollard: updates on package-feeds
        * Incorporated a few services
            * Look at feeds (pypi, npm, rubygems, etc)
            * Originally feed dependent. 
        * Been doing work to reduce some of the lossiness
        * Currently trying to incorporate work on identifying “top” packages in various repos (PyPI, npm, etc.) into package-feeds via list support. A lot has been been Google cloud centric, working to create more general containers
        * We’re doing some analysis: system calls, intend to do typosquatting
        * David W: Where’s your “top 100” lists & how you computed them?
            * Blog post: https://dlorenc.medium.com/hunting-for-malware-with-falco-834b19b398c9
* David W: We can help NIST by providing our definition(s) and categories of software
    * David S: Make a proposal e.g “The code in the kernel and the functions related to auth/escalated privilege” 
    * “Put a box around the things” 
        * Apply to everyone

    **Proposed NIST categories of critical software (directly disclosed during public NIST meeting)**:

* Operating systems hypervisors, containers 
* Network Control Software
* Identity, Credential, and Access Management (ICAM) software 
* Network Monitoring and Configuration Tools 
* Browsers 
* Remote Access and Configuration Management Tools
* Remote protocol-based scanning tools
* Endpoint Security Software

<h2>May 20, 2021</h2>


Attendees



* Kim Lewandowski (Google)
* Abhishek Arya (Google)
* Nuthan Munaiah (Applied Visions, Inc. Secure Decisions Division)
* Amir Montazery (Open Source Tech Improvement Fund)
* David A. Wheeler (Linux Foundation)
* David C Stewart (Intel)
* Jeff Mendoza (Google)
* Josh Smith (Codethink)
* Tom Pollard (Codethink)
* Daniel Silverstone (Codethink)
* Chris Lamb (Reproducible Builds)

Agenda



* (Jeff Mendoza) intro to github app: security improvements
* Called “Allstar” - gave demo
* Repo will be here: https://github.com/ossf/allstar
* (Amir) Executive Order work: Table-Top Exercises with workgroup: Collaborate on 2 objectives from the WH Executive Order that apply to workgroup
* [Table-Top Exercise](https://docs.google.com/document/d/1V8Qa5xRyL2PKB7kVOoWhFy5kVsIFfzxiBrF868r64PY/edit?usp=sharing): What is Critical Software?
* [Table-Top Exercise](https://docs.google.com/document/d/1jk7vsIjxUw1bE2TwoPqGDgE0sETpTKX-9nyIJ8bMUM0/edit?usp=sharing): List of Categories of Critical Software
* Please contribute your thoughts/comments to the documents
* (David Stewart) : Question about administrative separation in builds?  
* Kim mentioned Tekton: [https://github.com/tektoncd/community/blob/main/teps/0025-hermekton.md](https://github.com/tektoncd/community/blob/main/teps/0025-hermekton.md)

<h2>April 22, 2021</h2>


Attendees:



* Kim Lewandowski (Google)
* David C Stewart (Intel)
* Pierre Humberdroz (self)
* Amir Montazery (Open Source Tech Improvement Fund)
* Josh Smith (Codethink)
* Kees Cook (Google)
* Tom Pollard (Codethink)
* Daniel Silverstone (Codethink)
* Vinod Anandan (Citi)
* Hrvoje Bošnjak (Embcodeeng)
* Benjamin Font Pera (Embcodeeng)
* David A. Wheeler (Linux Foundation)
* Jenny Hoffman & Steven Randazzo (LISH)
* Nuthan Munaiah (Applied Visions, Inc. Secure Decisions Division)
* Jonathan Leitschuh (Gradle)
* Ed Vielmetti (Equinix Metal)
* Jake Sanders (Google)

Agenda



* (Josh Smith) [package-feeds](https://github.com/ossf/package-feeds) desired features discussion [[Proposal](https://docs.google.com/document/u/0/d/1QapExPEXLSKwqfGklvBv94HUMipFEiJza0z7ulMb868/edit)]
* (Kees Cook) raise awareness of by-default compiler [hardening features](https://wiki.ubuntu.com/ToolChain/CompilerFlags)
    * when you build project yourself, you may not know what the default compiler flags should be
    * Alpine and Ubuntu are only distros doing this
    * What about all the other distros?
    * Ideally this would be on in the upstream compilers themselves
        1. unlikely to be the default
        2. can be made an option at compiler-build time
    * social pressure through CVEs?
    * Next steps
        3. drive awareness
        4. come up with list of flags
        5. make options first class citizens in compiler self-test frameworks
        6. go to the distros
* (Kim) office hours?
    * every other meeting?
    * get questions in advanced?
    * publicize?
    * do we have enough or the right people to help?
* (David A. Wheeler) U of MN attacks on Linux kernel devs (experiments on non-consenting humans)
    * vast majority of patches have been made in good faith
    * one research project wrecked trust in UMN's work
    * [Revert of all umn.edu commits](https://lore.kernel.org/lkml/20210421130105.1226686-1-gregkh@linuxfoundation.org/)
* (Jonathan Leitschuh) - OSS Criticality Score
    * [security critical brainstorm](https://docs.google.com/document/d/1LQCeihQQ_N6phUSixfAJMUnu5XbTEBjChLFa3CwyWAw/edit#)
    * [How to Measure Anything in Cybersecurity Risk](https://www.goodreads.com/book/show/26518108-how-to-measure-anything-in-cybersecurity-risk)
    * related paper [Underproduction: An Approach for Measuring Risk in Open Source Software](https://arxiv.org/abs/2103.00352)
    * [Economics of Information Security](https://econinfosec.org/archive/weis2012/)

<h2>March 25, 2021</h2>


Attendees:



* Kim Lewandowski (Google)
* David C Stewart (Intel)
* David A. Wheeler (Linux Foundation)
* Florencio Cano (Red Hat)
* Vinod Anandan (Citi)
* Derek Zimmer (Open Source Tech Improvement Fund)
* Serkan Holat
* Chris Horn (Applied Visions, Secure Decisions division)
* Mike Malone (smallstep)
* Amir Montazery (Open Source Tech Improvement Fund)

Agenda



* (Oliver Chang) osv.dev demo and discussion
    * OSV is a vulnerability database & automation infrastructure for open source
    * Our thanks to Oliver Chang (Google), who is joining us at 7am Sydney time on his day off!
    * CVEs have many issues. Difficult to build automated tools
        * In particular: Lack of precise versioning, so you can’t match an exact version of a package that we’re using as a dependency
        * Requesting CVEs is also difficult, so many missed vulnerabilities
    * Their approach: osv.dev - precise versioning for each vulnerability, automation
    * Currently includes a few thousand vulnerabilities from OSS-Fuzz (fuzzing infrastructure)
    * They use commit IDs to identify which commit introduced the bug (via bisection)
    * David W: Bisect might sometimes be confused, e.g., if the API changes
    * Chris Horn: VCC = Vulnerability-contributing commit. It’s hard to find necessarily, can get many false positives
        * Another catalog of curated vulnerabilities: [http://vulnerabilityhistory.org/](http://vulnerabilityhistory.org/)
        * Tool to automate identification of VCCs: [https://github.com/samaritan/archeogit](https://github.com/samaritan/archeogit)
    * Oliver: The bisection is best effort.
    * Could you support package URLs (pURL)?
        * We think it’s something we could easily add. Currently we’re more focused on getting more data
    * How do you envision the data being used?
        * There’s nothing in OSS that checks if your pip packages have known vulnerabilities. There are proprietary tools that do that.
        * We want there to be a common toolsuite so anyone can determine if the OSS packages you use have known vulnerabilities.
    * We’re looking at NVD. We think there will always need to be humans to convert from CVEs (because they don’t provide adequate identification of what software is affected). We’re looking at Python - no community-maintained list.
    * David W: The Ruby community does have one, might be of interest
        * My understanding is that it’s really one person who does it on their spare time
    * Relationship with DWF? [https://github.com/distributedweaknessfiling/DWF-workflow/blob/main/FAQ.md#i-dont-have-a-github-account](https://github.com/distributedweaknessfiling/DWF-workflow/blob/main/FAQ.md#i-dont-have-a-github-account) 
    * URLs:
        * http://vulnerabilityhistory.org/
        * https://github.com/google/osv/issues/64
        * https://github.com/package-url/purl-spec
* OSTIF update on critical projects to secure (~10 minutes)
    * “Securing Critical Projects Audit Project” proposal
    * “We raise money (usually from commercial backers) to do security audits of OSS programs. We intend to create a proposal to identify critical projects that this group has identified as important”
    * “We use a bidding system that also considers qualifications, we try to impact the project as little as possible”
    * Used various data sources to create list. Shared inputs - included LISH (Harvard) report, OSSF Scorecard results, etc.
    * Plan to share the list later with the TAC. We spent a lot of time to create the list, don’t want someone else to “steal the list”
    * Kim: The OpenSSF doesn’t yet have a process for funding, though that’s in process.
    * David Stewart: We’ve identified a set of things we depend on in my company, we were thinking of cross-correlating with the “Underresourced” list. NOTE: We should target about 7 - 9 problem projects this year and focus on getting them cleaned up rather than boiling the ocean.
    * Chris Horn: If there’s infrastructure you can set up in projects to prevent future similar problems ( add static analysis tools, modifying design to be less likely in the future, etc.)
        * Derek: The goal is to help the project change so that they can counter similar problems themselves in the future. When you fix a fuzz problem, then the fuzzer has a chance to find new problems.
        * More manual review than pen testing
    * Andrey Khalyavin: What about non-security bugs
        * Derek: Sure, we find & fix those too. In OpenSSL we found significant performance bugs
    * Chris Horn: “Your methodology to develop a project list is  strong. The projects listed on Zerodium have a high market value; you should ensure your final list has good overlap. [https://zerodium.com/program.html](https://zerodium.com/program.html) “
        * @Chris Horn - Thank you for the feedback! You have brought up a good point in previous meetings about Zerodium and I think considering the value of zero-days is a good data point. After cross-referencing our project list and focus areas (based on review from research teams), along with the associated programming languages, I think we’re on the right track! See below: 
            * At least half of the project focus areas on the list includes: injection attacks, parser differences and error, escapes on incoming data and authentication, file upload, permission management and enforcement. Another good portion of them focus on parsers, logic bugs in request handling, path traversal, command injection. This is pretty consistent with what Zerodium looks for. 
            * The list is targeting two critical components of Apache. 
            * The list is looking specifically at drupal and joomla, along with two other PHP frameworks. These are on Zerodium’s payout list as well. 
    * What about Google Project Zero?
        * They do amazing work, we got the idea for OSTIF from Google Project Zero. What’s different is that we focus on specific OSS projects, they focus on whatever they choose.
        * We’d love to work with them, but they’re worried about conflicts of interest.
        * Kim: I might be able to help, please follow up with me
    * Kim: Often projects don’t need *code* help, it’s other stuff. Code reviews, documentation, etc. Then they can focus on the code. I’m interested in working on that.

<h2>March 11, 2021</h2>


Attendees:



* Kim Lewandowski (Google)
* Chris Lamb (Reproducible Builds / Debian)
* Abhishek Arya (Google)
* David A. Wheeler (Linux Foundation)
* Mike Malone (smallstep)
* Amir Montazery (Open Source Technology Improvement Fund) (OSTIF)
* Tara Tarakiyee (Open Technology Fund) (OTF)
* Pierre Ernst (Elastic)
* David C Stewart (Intel)
* Serkan Holat (self)
* Pierre Humberdroz (self)
* Derek Zimmer (Open Source Technology Improvement Fund) (OSTIF)

Agenda and notes:



* Presentation from Open Technology Fund (OTF)
    * OTF is a private non-profit organization funded by the US government
    * Focuses on providing internet freedom esp. In repressive countries, e.g., counter censorship
    * Has various funds available
    * “Internet Freedom Fund” includes critical infrastructure fund work (formerly they were separate)
        * Reproducible builds, human rights implications of 5G technologies, encrypting server name indication (SNI) to counter censorship
        * Collaboration between Princeton & Let’s Encrypt
        * PyPI improvements
    * Funding requests - no minimum, but must fit into their remit (anti-censorship tech, critical infrastructure, etc. - must be tech) - go to website to apply.- [https://www.opentech.fund/](https://www.opentech.fund/)
        * Intended to be straightforward & easy process
        * Two-stage process. (1) Validation (important, proposed by people who can do it, not a duplicate project, etc.). If okay, ask to submit a “full proposal”  with details on how it will be done.
        * OTF has limited funds, so need to ensure that money well spent
        * “Must serve people in repressive contexts - we’re the only organization that does exactly what we do.”
        * Many people propose good ideas but aren’t focused on human rights etc - can’t just fund general good ideas
    * OTF Application Form: [https://apply.opentech.fund/internet-freedom-fund-concept-note/](https://apply.opentech.fund/internet-freedom-fund-concept-note/)
        * More information on our fund here: [https://www.opentech.fund/funds/internet-freedom-fund/](https://www.opentech.fund/funds/internet-freedom-fund/)
        * [https://guide.opentech.fund/appendix-i-concept-note-guide](https://guide.opentech.fund/appendix-i-concept-note-guide)
    * Funding was paused last year, has been restarted this year.
    * Security audits
        * Many OSS projects cannot afford security audits, OTF can fund (if they’re important to those in oppressive regimes)
        * Red Team Lab - Various ways to fund: as OTF project, direct application to red team lab (not OTF funding but needs help in doing red team), “adversarial audit” (someone reaches out & finds concerns about it) - reverse engineer to see what data it communicates (it’s not common, but it leads to interesting things). E.g., Chinese Android app on Xi’s ideology requested root access [Washington Post]
    *  OTF contacts
        *  sarah@opentech.fund
        * Benjamin Mako Hill12:47 PM
        * tara@opentech.fund
    * 
* Kaylea Champion to present their paper “Underproduction: An Approach for Measuring Risk in Open Source Software”
    * It’s available here: [https://arxiv.org/abs/2103.00352](https://arxiv.org/abs/2103.00352)
    * It has a conceptual framework where it works to identify relative “underproduction”. They demo how to measure this by examining the time to resolution on Debian. This seems very related to the general problem of identifying “critical OSS projects that need help”.
    * Abstract: “The widespread adoption of Free/Libre and Open Source Software (FLOSS) means that the ongoing maintenance of many widely used software components relies on the collaborative effort of volunteers who set their own priorities and choose their own tasks. We argue that this has created a new form of risk that we call 'underproduction' which occurs when the supply of software engineering labor becomes out of alignment with the demand of people who rely on the software produced. We present a conceptual framework for identifying relative underproduction in software as well as a statistical method for applying our framework to a comprehensive dataset from the Debian GNU/Linux distribution that includes 21,902 source packages and the full history of 461,656 bugs. We draw on this application to present two experiments: (1) a demonstration of how our technique can be used to identify at-risk software packages in a large FLOSS repository and (2) a validation of these results using an alternate indicator of package risk. Our analysis demonstrates both the utility of our approach and reveals the existence of widespread underproduction in a range of widely-installed software components in Debian.”
    * From “Community Data Science Collective” which analyzes communities
    * This is the third in a series. First was Wikipedia, second was a literature review about software quality.
    * They were very inspired by Heartbleed vulnerability in OpenSSL.
    * Issue: important but low quality (“underproduction”)
    * Analyzed Debian GNU/Linux, used “speed of resolution” as measure of quality, popularity contest as view of importance (how many downloaded a package from users who supplied such data)
    * Kim L: We need to do better at identifying the critical projects. We have the OpenSSF criticality score, Harvard work, this, etc.

<h2>Feb 25, 2021</h2>


Attendees



* Kim Lewandowski (Google)
* Dan Lorenc (Google)
* Itay Shakury (Aqua Security)
* Derek Zimmer (Open Source Tech Improvement Fund)
* David A. Wheeler (Linux Foundation)
* Naveen Srinviasan(self)
* Mihai Maruseac (Google)
* David C Stewart (Intel)
* Chris Lamb (Reproducible Builds / Debian)
* Serkan Holat (self)
* Amir Montazery (Open Source Tech Improvement Fund)
* Jonathan Leitschuh (Gradle)
* Jonathan Meadows (Citi)
* Rendy jsv , a (self)

Agenda



* David A. Wheeler is “helping us fix the internet”! He basically works to improve the security of open source software. In the context of the OpenSSF, he helps the OpenSSF GB/TAC/members do what they’re trying to do. He works for the Linux Foundation. His title is really long: Director of Open Source Supply Chain Security. He leads the CII Best Practices Badge project. He also reads a lot, writes sometimes and goes to a lot of meetings! Kim says, “He fends off emails from Kim!”
* Australian friendly meeting time for March 25 (osv.dev demo)
* Tracee demo from Itay Shakury
    * [https://github.com/aquasecurity/tracee](https://github.com/aquasecurity/tracee)
    * More info: [https://blog.aquasec.com/ebpf-container-tracing-malware-detection](https://blog.aquasec.com/ebpf-container-tracing-malware-detection) 
    * It has two parts: “tracee eBPF” (which records events, a Linux kernel version 4.18 security tool), and “tracee rules” (this is the “rule engine” that triggers on events, e.g., potentially malicious behavior - this can be run separately). Can write signatures in rego (like the very popular Open Policy Agent (OPA)).
    * Could this be integrated into the malware analysis project? YES.
    * One problem with system calls is that when executing files, you get relative names which can be misleading. With eBPF (as used by tracee), we can get the actual absolute path of the file being executed & capture that more specific information.
    * Tracee: You can write in Go, which is a lot easier than writing in C. They also support “rego”, a Datalog-based DSL; for more on rego see: [https://www.openpolicyagent.org/docs/latest/policy-language/](https://www.openpolicyagent.org/docs/latest/policy-language/)
    * This relates to our WG charter because owners of package feeds like PyPI and others such as Gradle can identify the problem packages.
    * Modulecounts.com counts packages - down right now
* Typosquatting
    * Kim is talking with academics
    * David: Levenshtein distances are a common way to measure word similarity, which can help detect many typosquatting attacks. PostgreSQL has a way to index for Levenshtein distances (via Soundex), which might make many similarity matches MUCH faster (e.g., to detect typosquatting). More info: [https://info.crunchydata.com/blog/fuzzy-name-matching-in-postgresql](https://info.crunchydata.com/blog/fuzzy-name-matching-in-postgresql)
    * David: May also be able to detect “these 2 programs are similar” (e.g., one may be a subverted version of the real one) by examining code diffs and/or description text similarities. There are many ways to detect similarities, e.g.: [https://medium.com/@adriensieg/text-similarities-da019229c894](https://medium.com/@adriensieg/text-similarities-da019229c894)
    * “Claiming a namespace” is a model
    * Maybe a Kaggle model?? Problem: We have to have known bad data
* Measuring criticality - we have 2 ongoing projects:
    * criticality-score project
    * LF/LISH (Harvard) work
    * There’s also a brainstorming document
* Project updates:
    * Package Feeds - watches package managers, reports changes
    * Malware Analysis - currently uses some Falco rules to detect malicious attacks. It’s pluggable, so we can integrate many components. Currently logs files opened during install. Currently analysis is manual, but we actually found some things. Working on automation. Goal: Find a way to get it productionized so it continuously run.
    * plans and ideas for the project [here](https://docs.google.com/document/d/1Noq2_1zlBx7X9LCXRgpzyKMqTyQTcvHHcGKTjV6UyOg/edit) “Project "Galaxy Watcher" AKA Package-Feeds”
    * As long as there’s a documented API, the OpenSSF “Security Metrics” dashboard can bring that data in
* [“security-critical” packaSecurity-Critical Packages Brainstormge brainstorm doc](https://docs.google.com/document/d/1LQCeihQQ_N6phUSixfAJMUnu5XbTEBjChLFa3CwyWAw/edit#heading=h.qmm2n8g4h2ql)

<h2>Feb 11, 2021</h2>


Attendees



* Kim Lewandowski (Google)
* David A. Wheeler (Linux Foundation)
* Nuthan Munaiah (Secure Decisions)
* Mihai Maruseac (Google)
* Derek Zimmer (OSTIF.org)
* Mike Malone (smallstep)
* Chris Lamb (Reproducible Builds)
* Martin Carnogursky (self)
* Naveen Srinivasan (self)
* Pierre Ernst (Elastic)
* Kate Stewart (LF)
* Serkan Holat (self)
* Ryan Haning (Microsoft)
* Vinod (self)
* David C Stewart(Intel)
* Jonathan Meadows (Citi)

Agenda:



* Reminder: Upcoming OpenSSF Town Hall Meeting on Monday, February 22, 1:00-2:00p ET.
    * Register here: [https://zoom.us/webinar/register/WN_5iCAH2-ETaGpiI7UQNSMXw](https://zoom.us/webinar/register/WN_5iCAH2-ETaGpiI7UQNSMXw)
    * All WG leads must fill in a status update by FRIDAY Feb 12 so that the planning committee can review them on Feb 15. Please help the leads do this!
* Talking about the Reproducible Builds Project and opportunities to improve the widespread adoption of verifiable software builds. (Chris Lamb +/- Derek Zimmer) - [https://reproducible-builds.org/](https://reproducible-builds.org/)
    * Chris Lamb: Has been doing OSS 10+ years professionally. Debian Project Leader for a few years, OSI Board. Lead of Reproducible builds projects.
    * “Reproducible builds is a terrible name for a great idea”
        * We’ve collected many examples of attacks that would have been countered by reproducible builds
        * Key recent example - SolarWinds - Referenced from Linux Foundation blog post: [https://www.linuxfoundation.org/en/blog/preventing-supply-chain-attacks-like-solarwinds/](https://www.linuxfoundation.org/en/blog/preventing-supply-chain-attacks-like-solarwinds/)
        * You might be compromised & not really know it; reproducible builds can help counter that too!
    * Reproducible builds - what is it?
        * You can always review source code for vulnerabilities (intentional or unintentional), but most people ingest *pre-packaged* code, not the source code directly
        * If the pre-packaged code doesn’t correspond to the source code, then it doesn’t matter what the source code review did, because they don’t correspond.
        * In reproducible builds, can reproduce a build to produce the same result, showing that the built result & the source code correspond
    * Dave @ Intel: What’s the small kernel of truth to explain what it does for me?
        * “This shows that the binary does not correspond to the source”
        * It’s a hard thing to get reproducible builds, yes?
    * You can often find vulnerabilities in the source code from reproducible builds
        * Rebuilding can show “things that shouldn’t be there”
    * What makes it easy or hard?
        * Technical vs. social changes matter
        * Some projects are easy, some are hard, depending on their stack.
            * A small C program is generally very easy to make reproducible
            * Getting that into the upstream project can be very hard, depending on how important the project thinks it is (“what’s the benefit to me?”)
        * Toolchains dramatically make it easier or harder
            * Rust toolchain difficult currently
            * .NET on Linux difficult (lots of randomness)
        * Yocto’s been good
        * Making an entire distribution is hard to make reproducible.
            * Tails *IS* reproducible. (OS for privacy. Includes Tor, based on Linux.) Tails is famous because Snowden used it. That means that there’s a lot more trust in their build servers. [https://tails.boum.org/contribute/build/reproducible](https://tails.boum.org/contribute/build/reproducible)
        * Any documentation on “how to make project reproducible”
            * No perfect page, see: [https://reproducible-builds.org/docs/](https://reproducible-builds.org/docs/)
            * Documentation is one of our weaker points right now
        * Kate Stewart: I’d love to get Zephyr (kernel) to this stage, and avoid regressions. 
        * David Wheeler: To make it reproducible
            * #1 - actually try to make it reproducible!
            * #2 - check for reproducibility in your CI pipeline to prevent regressions (do it twice)
        * David Challenges:
            * Timestamps, uninitialized values, “random” orders that need some ordering imposed on them
        * Most common challenges:
            * Chris Lamb - it’s a lot of little things. Timestamps, Filenames from directory
        * Who is best to talk to in package managers?   Pypi, etc. 
            * Most are coming from own distributions ( Debian, Arch, Suse, etc. )
            * No one is doing cross-distribution, they’re focused on specific things
            * PyPI etc same problem
        * The same problems happens with scripting languages.
            * Python - download built packages from PyPI
            * Ruby - download built gems from RubyGems
            * JavaScript - download built packages using npm
        * What about kernel-imposed randomness?
            * Address Space Layout Randomness (ASLR) - no problem. That’s a load-time thing
            * Kernels want to get signed binaries, with cryptographic signatures. You can’t but the *signature* in, that’s a bigger challenge (just don’t repro)
            * Preload - no problem, just do the checks before this optimization
        * Bigger problem: Profile-guided optimization
            * Run the program at the same time, optimize common paths
            * It makes it hard to reproduce later, because the choices are nondeterministic (could be varied depending on load of build system). People just “turn on EDO” which causes problems
        * What about GCC itself (GNU toolchain)? Does it reproduce?
            * Some nits, that need to be work around (--seed).   Believe it to be but it needs to be confirmed.
        *  David: Heirarchy -  source code,  tools,...
            * Reproducible builds confirms that packaged version matches the source code IF the build tools are not subverted.
            * This isn’t a “turtles all the way down”. If you’re worried about subverted build tools, this is the “trusting trust” problem. Subverted build tools can be countered by either bootstrappable builds and/or diverse double-compling (DDC). But currently the worry is less subverted build tools (this is harder attack), but subverted build processes (which SolarWinds etc. have shown is QUITE possible). There’s no point in trying to counter malicious build tools if builds can be subverted (without detection) in the first place.
        * What about doing diff’s between build results? Might see malicious insertions
            * Yes, I’ve done that. When we do updates, look at software changes.
            * “Diffoscope” can be very helpful in examining differences. Useful for self-auditing
            * I’ve also done it with updated firmware.
            * Diffoscope is really good at this!
        * Wheeler: Diffoscope is useful, but it takes expertise to understand what you’re looking at. That kind of comparison is probably only practical for important cases, while “make everything reproducible builds” is plausible
        * Gradle - recognition that must disable build caches so that you’re actually rebuilding
        * What needs to change to get more programs to be reproducible builds?
            * Want tools on their platforms to check if their software is reproducible. When developers themselves can easily check, they start doing it (similar to fuzzing)
            * Have all the big CI (GitHub, GitLab) had a simple “am I reproducible” would be a big deal.
            * Financial incentives? If you want to incentivize OSS projects to do something, find a way to put money on it. A pull request from an outsider won’t help often, because they don’t understand the bigger picture. Funding that work can help.
            * What about having the package repos rebuilding everything & telling if the built system produces the same thing?
                * PyPI, npm, system package manager
            * If you Google a confusing dependency, someone was able to insert a malicious library…. Will try to learn more
            * Marketing/mindshare - we’re volunteers, not good at trumpeting our work
            * It would be REALLY valuable if GitHub & GitLab said “this is reproducible” or “this is not reproducible”
        * Could this be solved by creating a container that tweaked by kernel system calls? E.g., always same time.
            * ?: I’ve tried it.
            * You could use LD_PRELOAD to intercept many system & library calls on Linux.
            * Wheeler: I’m skeptical it would work well. If time result’s always the same, many builds will fail (they expect later timestamps to happen later). Most larger builds run in parallel for speed, and that creates hardware non-determinism (you could force things to be done sequentially, but that’s MUCH slower).
            * dettrace: Beijing Technology. They did LD_PRELOAD & could resolve some parallelism. [https://github.com/dettrace/dettrace](https://github.com/dettrace/dettrace). But this is a  prototype, currently only works for x86-64 Intel CPUs.
* Package Feeds/Malware Analysis Project Organization/Repos - ??? URL HERE
* In two weeks: Demo of traceee

<h2>Jan 28, 2021</h2>


Attendees



* Kim Lewandowski (Google)
* Chris Horn (Secure Decisions)
* Nuthan Munaiah (Secure Decisions)
* Martin Carnogursky (self)
* David A. Wheeler (Linux Foundation)
* Ryan Haning (Microsoft)
* Mihai Maruseac (Google)
* Mike Malone (smallstep)
* Jordan Wright (Self)
* Dan Lorenc (Google)
* Abhishek Arya (Google)
* Amir Montazery (OSTIF)

Agenda



* (Amir) discuss findings with the Linux Kernel security review - [Doc for workgroup](https://docs.google.com/document/d/1WrciT0NJfOPZXfQw5cZTOYsHLnQwUxjFsT3Jh96u-Ks/edit?usp=sharing)
* <malware analysis open discussion!>
    * Dan to demo Falco version - this can detect some malicious activity during install time
    * [jordan] Quick update on package-feeds: Ready to merge Cloud Run transition, terraform will be done this evening.
    * [Wheeler] Check out "Backstabber's Knife Collection: A Review of Open Source Software Supply Chain Attacks" (2020) [https://arxiv.org/abs/2005.09535](https://arxiv.org/abs/2005.09535) - the authors have created a collection of malicious software that could be used for testing
    * [https://github.com/rsc-dev/pypi_malware/tree/master/malware](https://github.com/rsc-dev/pypi_malware/tree/master/malware)
* (Nuthan) Is criticality score a proxy for popularity? See [Google Group Conversation](https://groups.google.com/g/wg-securing-critical-projects/c/wi09IkuTfIU)
    * On a related note, is the criticality score of a repository correlated with its [reaper](https://reporeapers.github.io/results/1.html) score?
    * A good counter-example is sudo. It just had a vulnerability publicly reported, and it’s critical for security, but it’s not a “popular” repo.
* (Chris Horn) Suggestions for identifying critical software
    * Idea 1: monitor exploit markets for price willing to be paid
        * Example: High pay-out software listed on [Zerodium](https://zerodium.com/program.html) is probably critical
    * Idea 2: coordinate and/or collaborate with NTIA (Dr. Allan Friedman) and MITRE (Bob Martin) around software bill of materials (SBOM) formats. Graph analytics that [Martin Čarnogurský](https://github.com/SourceCode-AI/aura) performs on dependencies may be applicable.
        * NTIA is working on this, see: [https://www.ntia.gov/SBOM](https://www.ntia.gov/SBOM)
        * [Web search for SBOM, SPDX, and CycloneDX](https://duckduckgo.com/?t=ffab&q=sbom+spdx+cyclonedx&ia=web)
        * [https://github.com/CycloneDX/specification](https://github.com/CycloneDX/specification); [https://cyclonedx.org/tool-center/](https://cyclonedx.org/tool-center/) 
        * [https://spdx.github.io/spdx-spec/7-relationships-between-SPDX-elements/](https://spdx.github.io/spdx-spec/7-relationships-between-SPDX-elements/)
    * [jwright] Similar to BOM, libraries.io has some interesting experiments that are worth considering [https://libraries.io/experiments](https://libraries.io/experiments) 
        * A challenge of dependency analysis is that dependencies take on multiple technical forms:  programming language referenced package, direct execution, nested dependency, more? 
    * [EU FOSSA](https://ec.europa.eu/info/departments/informatics/eu-fossa-2_en) identified 14 projects, in alphabetical order: 7-zip, Apache Kafka, Apache Tomcat, Digital Signature Services (DSS), Drupal, Filezilla, FLUX TL, the GNU C Library (glibc), KeePass, midPoint, Notepad++, PuTTY, the Symfony PHP framework, VLC Media Player, and WSO2.
* Can we (re)build & verify build? That is, verified reproducible builds
    * Chris Lamb will present on that, scheduled for February
    * David: I think reproducible builds are great, but you have to work at it. Embedded timestamps, force order of collections.
    * Martin Čarnogurský : I’m working on Python to get reproducible builds. Working but no one using it. Installs aren’t supposed to run code, but everyone does. [admin@sourcecode.ai](mailto:admin@sourcecode.ai)
        * [https://github.com/SourceCode-AI/aura](https://github.com/SourceCode-AI/aura) 
    * Dockerhub: “verified builds” - they (third party) built it for you. Not the same as reproducible builds
* [Kim] Once we identify critical projects, how do we improve projects?
    * Perhaps a tiering system, what criteria gets you what badge?
    * Perhaps a bounty program: if you earn X you get $$. (Have to be limited to specific projects or total amount)

<h2>Jan 14, 2021</h2>


Attendees



* Kim Lewandowski (Google)
* Jenny Hoffman (Lab for Innovation Science at Harvard - LISH)
* Mike Malone (smallstep)
* Martin Carnogursky (self)
* Amir Montazery (Open Source Technology Improvement Fund) 
* Kate Stewart (LF)
* Derek Zimmer (Open Source Technology Improvement Fund)
* Mihai Maruseac  (Google)
* Lauri Ojansivu (xet7 / Maintainer of Wekan [https://wekan.github.io](https://wekan.github.io) )
* Chris Horn (Secure Decisions)
* Nuthan Munaiah (Secure Decisions)

Agenda



* [Criticality Score](https://github.com/ossf/criticality_score) (Abhishek)
    * [Slides](https://docs.google.com/presentation/d/1amcmu0RwmhZlYytK52i0OZ6cf3Id_TGcIhVRn7grPjc/edit?usp=sharing)
* Brainstorming how to quantifying prevalence & usage of OSS beyond the library/component level (Jenny Hoffman)
    * [Proposed Framework for Expanding Census Scope  ](https://docs.google.com/document/d/1RCAglan07WRWoU_5xZg6COnM_MBIYMzZkmesYEnxECg/edit?usp=sharing) 

<h2>Dec 3, 2020 </h2>


Attendees



* Kim Lewandowski (Google)
* Ryan Haning (Microsoft)
* Dustin Ingram (Google)
* Jordan Wright (Self)
* Amir Montazery (ostif.org) 
* Ashish  Bijlani (Ossillate)
* Jenny Hoffman (Lab for Innovation Science at Harvard (LISH)
* Mike Malone (smallstep)
* David A. Wheeler (Linux Foundation  (LF))
* Kay Williams (Microsoft)
* Maciej Mensfeld (Castle.io, Diffend.io)
* Martin Carnogursky (self)
* Altaz Valani
* Mike Malone
* Dan Lorenc (Google)
* Derek Zimmer (ostif.org)

Agenda



* Malware detection!
    * Intros/what are other people working on here?
        * Maciek
            * Working on security analysis for RubyGems.
            * Diffend.io is currently running alongside RubyGems.
            * [https://diffend.io/](https://diffend.io/)
                * Also uses yanked library versions from RubyGems (each gem release is being downloaded).
                * Behavior and static analysis.
                * Also can track behaviors when running in a full Rails environment
                    * Still missing certain cases (only Windows, only specific versions, etc.)
                * Assigns a Risk score, RubyGems maintainers get notified when something is over a certain threshold.
                    * Many of the solutions are pretty specific to Ruby & Rails ecosystem.
                * Working on a system to notify maintainers via opening GitHub issues both about security and quality issues.
                * Most things don't get CVEs nor do they get reported to bundler-audit, data is only available to RubyGems mostly (if problematic,  “yank” to discourage loading it).
                    * Things get "yanked", don't get reported to bundler audit.
                    * Did a CVE report a year ago, still hanging, not interested.  “CVE is not user friendly”. Months later still not published. CVE didn’t want to publish typosquatting, “malicious by design is not a vulnerability”, 
                    * Bundler-audit doesn’t get updates for CVSS scores for Ruby.
                    * Bundler-audit won’t include gray areas, and it runs too late… need to counter on “bundle install”.
                    * Diffend.io notifies about usage of abandoned gems as well as it allows to define organization wide packages usage rules.
                    * “Normal” non-malicious Ruby gems usually don’t run code on install,  though they CAN. They need to do it for native (C code) extensions,  some companies  download some things. That seems different from Python, where you  basically always run code on install of a package.
                * This tracks activities on running actual code, not just during install. To “run” it, it simply “requires” it & waits for a while. Then creates a Rails app,  etc., to see if it does something weird.
                * Many gems send weird data, probably for analytics. E.g., send data on install.
                * Presentation: [https://www.youtube.com/watch?v=wePVhZeZTNM&feature=emb_logo](https://www.youtube.com/watch?v=wePVhZeZTNM&feature=emb_logo)
                * “It’s better to be strict (as a company) about what you accept in.”
                * Some things (signals) are specific to Ruby
                * E.g., JavaScript publications are often of only pre-compiled data which makes static analysis and manual review more problematic.
        * Martin Čarnogurský is working on something similar for Python to detect malware in Python packages. URL: [https://github.com/SourceCode-AI/aura](https://github.com/SourceCode-AI/aura)
            * Also created “diffing” engine (inspired by diffoscope) to determine if Python distributed is reproducible from claimed GitHub repo - diffs packages (different version) or package <-> repository
            * Aura includes reproducing check (based on reproducible-builds.org). (WIP)
            * Often the MD5 declared != actual package included. Hints that package was manually edited.
            * Currently few Python packages are reproducible.
            * Leaked credentials are very common. (Common for Ruby also.) There should be a way to warn people.
            * Aura is PURELY static. It does not diff dynamic execution results. Works to NOT run code at all. Wheels should not be executing code via setup.py but everybody is doing that.
        * RubyGems doesn't have a lot of resources today.
            * Not commercial
            * “Ruby Together” kinds funding from various organizations
        * PyPI similar situation - some resources, not a lot
        * Jordan's [update from slack](https://openssf.slack.com/archives/C019Y2AR76J/p1607012375119800)
    * We have a GCP project setup to run stuff
    * A repo for feed parsing: [https://github.com/ossf/package-feeds](https://github.com/ossf/package-feeds)
    * Jordan's repo: [https://github.com/jordan-wright/ossmalware](https://github.com/jordan-wright/ossmalware)
    * What's next?
    * Other URLs:
        * [https://github.com/rubysec/ruby-advisory-db](https://github.com/rubysec/ruby-advisory-db)
        * [https://cdn.sourcecode.ai/pypi_datasets/index/datasets.html](https://cdn.sourcecode.ai/pypi_datasets/index/datasets.html)
    * [Criticality Score](https://github.com/ossf/criticality_score) project -- feedback / PRs welcome! :)

<h2>Nov 19, 2020 </h2>


Attendees



* Kim Lewandowski (Google)
* Ryan Haning (Microsoft)
* Dustin Ingram (Google)
* Jordan Wright (Self)
* Amir Montazery (ostif.org) 
* Derek Zimmer, Open Source Technology Improvement Fund

Agenda



* Internet Security Research Group (ISRG) preso (Josh). Nov 19
    * [slides](https://docs.google.com/presentation/d/1alJI99oAPFlt_NBh52fnIB_A7WfXjvhjLeLklABQWdo/edit?usp=sharing)
* [Dynamic analysis](https://jordan-wright.com/blog/post/2020-11-12-hunting-for-malicious-packages-on-pypi/) looking for malware (Jordan)

<h2>Nov 5, 2020 Meeting Canceled</h2>


<h2>October 22, 2020</h2>


Attendees



* Derek Zimmer, Open Source Technology Improvement Fund
* Amir Montazery, OSTIF
* David A. Wheeler, Linux Foundation
* Kim Lewandowski, Google
* Ryan Haning, Microsoft
* Mike Malone, smallstep
* Jenny Hoffman, Laboratory for Innovation Science at Harvard/Core Infrastructure Initiative

Agenda



* OSTIF preso (Amir Montazery)
    * [Link to presentation](https://docs.google.com/presentation/d/e/2PACX-1vS3DcMkHQYIvMoea0XWEd68JMcOIKaBjLPAhHzncZPKYq0kQ3-KazmvurHACU5LtSOdMxanatlOls1I/pub?start=false&loop=false&delayms=3000)

<h2>October 8th, 2020</h2>


Attendees



* Derek Zimmer, Open Source Technology Improvement Fund
* David A. Wheeler, Linux Foundation
* Craig Peters, Microsoft
* Kim Lewandowski, Google
* Amir Montazery, OSTIF
* Brian Caswell, Microsoft
* Ryan Haning, Microsoft
* Emily Ratliff, IBM
* Mike Malone, smallstep
* Jenny Hoffman, Laboratory for Innovation Science at Harvard

Agenda



* [LISH (Harvard) gives update on CII Survey](https://docs.google.com/presentation/d/1IdfRJIeIItSkTTEksmAUZG8_6lUuMv5VvwpGvB3oD7I/edit?usp=sharing) (Jenny Hoffman) ~15 min

<h2>September 24th, 2020</h2>


[Recording](https://www.youtube.com/watch?v=PGwFyzh2KTA)

Attendees



* David A. Wheeler, Linux Foundation
* Derek Zimmer, Open Source Technology Improvement Fund
* Dmitry Vyukov, Google
* Craig Peters, Microsoft
* Amir Montazery, OSTIF
* Frank Nagle, Harvard Business School
* Jenny Hoffman, Laboratory for Innovation Science at Harvard 
* Taylor McCaslin, GitLab
* Ryan Haning, Microsoft
* Kay Williams, Microsoft
* David C Stewart, Intel
* Saagar Saini, Google
* Kim Lewandowski, Google
* Dustin Ingram, Google

Agenda



* Swag (including a T-shirt!) is available at: https://store.openssf.org/
* (Dmitry Vyukov) “The state of the Linux Kernel security”
* Merge LF and CII Projects (see [full proposal](https://docs.google.com/document/d/1fKoriYszCEYuNsj1v44LxfBmLEuMqjos21Yc_iEGB0w/edit?usp=sharing))?
    * [CII Census](https://github.com/ossf/wg-securing-critical-projects/issues/14)
    * CII Survey
    * Security work for OpenSSH/OpenBSD/Linux Kernel (not OpenSSL; CII did fund OpenSSL, but that funding ended some time ago)
    * Proposal, as briefly described in [https://github.com/ossf/wg-securing-critical-projects/issues/14](https://github.com/ossf/wg-securing-critical-projects/issues/14), is to move this existing work so it reports to / belongs to this WG
    * Future work will require funding. By itself accepting this doesn’t create funding, but it creates a place to discuss future funding. Once these are moved into the WG, discussions about future funding needs / priorities can be discussed, and then raised to the OpenSSF GB for discussion.
    * AIl: please participate in [offline vote](https://doodle.com/poll/gakqkanvp92e3qn5)  < [https://doodle.com/poll/gakqkanvp92e3qn5](https://doodle.com/poll/gakqkanvp92e3qn5) >, voting closes by end of next Tuesday (2020-09-29) Eastern Time 11:59pm (unless someone prefers a different end time!). The question says: “Should OpenSSF accept CII Census, Survey, & special projects?” The question is short, because Doodle does not permit long questions.  The “Special projects” are the security efforts for OpenSSH/OpenBSD/Linux Kernel. The poll question just says “OpenSSF” (to make the question short), but if this poll is accepted, all 3 of these would become part of the “Securing Critical Projects” WG.

<h2>September 10th, 2020</h2>


[Recording](https://www.youtube.com/watch?v=z_b8pp-nmog) 

Attendees:



* Ryan Haning, Microsoft
* Joshua Lock, VMware
* Amir Montazery, OSTIF
* Jenny Hoffman, Laboratory for Innovation Science at Harvard (LISH) [jhoffman@hbs.edu](mailto:jhoffman@hbs.edu) 
* Frank Nagle, Harvard Business School / LISH [frank@hbs.edu](mailto:frank@hbs.edu) 
* Cristina Muñoz
* [Kim Lewandowski](mailto:kim.m.lewandowski@gmail.com), Google

Agenda



* [preso from Harvard Census II](https://drive.google.com/file/d/1We7aYXDDiyoEhhZCApa9xqKvFtDqLGHa/view?usp=sharing)
* WG Mission
    * [brainstorming doc](https://docs.google.com/document/d/1qPLlxL816VmRCvK4_TqYeKB-fRZee595tdB6JjWuiyY/edit#)
* Can we start with ~5 projects?
    * Linux?
* "Menu of Services" - dlorenc
    * Create [shared doc to brainstorm](https://docs.google.com/document/d/1Q96lBbH8oPpm97-uiPdTIqRFAGzIqiNUw5Xjd8Vftaw/edit#) on what we could offer. 
* Relationship between this WG and other WGs

<h2>August 27th, 2020</h2>


[Recording](https://www.youtube.com/watch?v=MJYG6uHQmo4)

Attendees:



* [Kim Lewandowski](mailto:klewandowski@google.com) (Google) - Bay Area!
* David A. Wheeler (Linux Foundation)
* Mike Malone (Smallstep)
* Dan Lorenc (Google)
* Dan Middleton (Intel)
* Nicko van Someren (Absolute Software)
* Joshua Lock (VMware)
* Cristina Munoz (independent)
* Ryan Haning (Microsoft)
* Niroshan Rajadurai (GitHub)
* Amir Montazery (OSTIF)
* Shahidul Hoque (Dell)
* Michael Dolan (LF)
* Keanan McArthur (independent)

Agenda:



* Intros!
    * Where are you located? Why are you interested in this WG?
* Logistics
    * Meeting cadence, time?
        * Monthly seems ok
    * Google Meet? Zoom? Skype? Microsoft Teams?
        * 
    * OneDrive? Google Drive?
    * Mailing list? Github Discussions?
        * [https://groups.google.com/g/wg-securing-critical-projects](https://groups.google.com/g/wg-securing-critical-projects)
    * Chat: Slack channel?
        * +1’s for Slack
    * Meeting recordings!
* Scope and Goals for the group.
    * “Open Source projects that have been identified as needing help from the foundation can access a shared pool of security engineers (fireperson team aka FIRE: Free Incident Response Engineers) that could move from project to project. These engineers can also provide mentoring to the various projects they are assisting. This shared pool can also be responsible for building and running services (package repositories, etc.) that are needed to support secure software development.”
    * [www.coreinfrastructure.org](http://www.coreinfrastructure.org)
        * 2 ways to get funding
            * CII had an analysis of what’s important to fund (push)
                * [Nicko] pushback was that projects didn’t like to be told to do specific activities
            * Grant model had many problems
                * [David] projects that needed it the most often didn’t make requests
                * [Nicko] a lot of projects didn’t know they could apply for grants
                * [Nicko] some pushback that projects didn’t want to use the money for specific changes
                * [Nicko] funded a number of security analysis efforts so projects had an idea of where they could improve
                * [David] Grant models can work, and might be good longer term, but they take a lot of time - in the shorter term, better to identify what’s most important and focus on that
    * What projects would people like to see improved?
        * Harvard Census II – https://www.coreinfrastructure.org/programs/census-program-ii/
    * What improvements would we make?
* [David A. Wheeler] Overview of CII Best Practices Badge (~10min) (CII Census II is relevant, probably a future meeting)
    * Over 3.4k projects now
    * If you contribute to an OSS project, encouraged to get a badge: [https://bestpractices.coreinfrastructure.org](https://bestpractices.coreinfrastructure.org)
    * Badge Project statistics: [https://bestpractices.coreinfrastructure.org/project_stats](https://bestpractices.coreinfrastructure.org/project_stats)
    * Full presentation available on best practices project here: [https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/cii-bp-badge-intro.pptx](https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/cii-bp-badge-intro.pptx)
    * Taking steps for securing critical projects
    * Identify critical projects comes 1st
    * Next: Triage: which critical projects are in good shape vs. concerning? Might want to prioritise “critical & concerning” projects. May still want to work on critical projects that are less concerning, but the work to be done is probably different
    * Must then decide what to do with critical projects
    * Funding model will vary project to project
        * [Dan] we could offer a “menu” of services
            * Brainstorm: Funding 3rd pty devs, infrastructure credits, expert reviews, security audits, security evangelist pool,
            * [Dan] - could we reach out to maintainers and ask them for their wishlist?
        * [Amir] advocates for in depth analysis of projects
        * [Joshua] should consider how to secure critical projects without funding or post-funding; how do we sustainably secure critical projects?
        * [David] May want to ask them to do certain practices (test, etc.)
* Next steps
    * Will try to get Harvard to present in ~ 2 weeks to present on CII Census II
    * In ~4 week schedule next meeting
    * Start with defining critical projects
    * Set up slack channel
